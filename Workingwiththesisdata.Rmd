---
title: "s&ds thesis stuff"
author: "Matthew Ross"
date: "2025-02-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


```{r}
library(tidyverse)
library(readxl)
library(tidytext)
library(dplyr)
library(wordcloud)
library(stringr)
library(purrr)
library(igraph)
library(tidyr)
library(ggplot2)
library(lubridate)
library(xts)
library(PerformanceAnalytics)
library(conflicted)
conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)

# Read in your data files
lpdata <- read.csv("/Users/mross/Downloads/master.csv")
funddata <- read_excel("/Users/mross/Downloads/Preqin_Fundperformance_export-28_Feb_2530ee1808-842d-4444-9364-5c85226b4815.xlsx")


colnames(funddata)

# Subset funddata to keep only the needed columns
funddata_sub <- funddata %>%
  select(
    `FUND ID`,
    `ASSET CLASS`,
    `FINAL CLOSE SIZE (USD MN)`,
    `FUND MANAGER`,
    `CORE INDUSTRIES`,
    `FUND MANAGER TOTAL AUM (USD MN)`,
    `INDUSTRY VERTICALS`,
    `FUND NUMBER (OVERALL)`,
    `TARGET SIZE (USD MN)`,
    `MEDIAN BENCHMARK NET IRR (%)`,
    `MEDIAN BENCHMARK NET MULTIPLE (X)`,
    `MEDIAN BENCHMARK RVPI (%)`,
    `AVERAGE BENCHMARK NET IRR (%)`,
    `AVERAGE BENCHMARK CALLED (%)`,
    `AVERAGE BENCHMARK DISTRIBUTED (%) DPI`,
    `AVERAGE BENCHMARK NET MULTIPLE (X)`,
    `AVERAGE BENCHMARK RVPI (%)`,
    `WEIGHTED BENCHMARK NET IRR (%)`,
    `MEDIAN BENCHMARK DISTRIBUTED (%) DPI`,
    `WEIGHTED BENCHMARK CALLED (%)`,
    `WEIGHTED BENCHMARK DISTRIBUTED (%) DPI`,
    `WEIGHTED BENCHMARK NET MULTIPLE (X)`,
    `WEIGHTED BENCHMARK RVPI (%)`,
    `POOLED BENCHMARK NET IRR (%)`,
    `MEDIAN BENCHMARK CALLED (%)`,
    `BENCHMARK NAME`,
    `GEOGRAPHIC FOCUS`,
    `STRATEGY`,
    `FUND NUMBER (SERIES)`
  )

length(unique(lpdata$FUND.ID))

# Left join so that each lpdata row gets the matching funddata info
lpdata_joined <- lpdata %>%
  left_join(funddata_sub, by = c("FUND.ID" = "FUND ID"))

# Now, print the rows from lpdata that did not have a matching funddata row.
# (Here, we check one of the funddata columns; if it is NA, no match was found.)
missing_matches <- lpdata_joined %>% filter(is.na(`FUND MANAGER`))
print(missing_matches)

df <- lpdata_joined

sum(is.na(df$`INDUSTRY VERTICALS`))

df <- df %>% select(-c(`INDUSTRY VERTICALS`))


preqindata <- read.csv("/Users/mross/Downloads/LP data new new.csv")


colnames(preqindata)
# Define the columns to remove. Using ncol(preqindata) ensures that from column 579 onward is removed.
cols_to_remove <- c(295, 296, 341:458, 467:500, 503:504, 521:530, 547:550, 561:570, 573:574, 579:ncol(preqindata))

# Create a new dataset that excludes these columns
preqindata_new <- preqindata[, -cols_to_remove]
colnames(preqindata_new)

```
Clean up column names

```{r}

# Define vectors of detailed columns (these names remain unchanged)
cols_to_drop_buyout <- c(
  # BuyoutDeals Group 1: Energy
  "BuyoutDeals_PrimaryIndustry_Oil...Gas_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Oil...Gas_AggValue" ,
  "BuyoutDeals_PrimaryIndustry_Power...Utilities_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Power...Utilities_AggValue",
  "BuyoutDeals_PrimaryIndustry_Renewable.Energy_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Renewable.Energy_AggValue",
  "BuyoutDeals_PrimaryIndustry_Energy.Storage...Batteries_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Energy.Storage...Batteries_AggValue",
  
  # Group 2: Healthcare & Life Sciences (Buyout)
  "BuyoutDeals_PrimaryIndustry_Healthcare_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Healthcare_AggValue",
  "BuyoutDeals_PrimaryIndustry_Healthcare.IT_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Healthcare.IT_AggValue",
  "BuyoutDeals_PrimaryIndustry_Healthcare.Specialists_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Healthcare.Specialists_AggValue",
  "BuyoutDeals_PrimaryIndustry_Medical.Devices...Equipment_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Medical.Devices...Equipment_AggValue",
  "BuyoutDeals_PrimaryIndustry_Pharmaceuticals_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Pharmaceuticals_AggValue",
  
  # Group 3A: Digital & IT (Buyout)
  "BuyoutDeals_PrimaryIndustry_Software_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Software_AggValue",
  "BuyoutDeals_PrimaryIndustry_IT.Infrastructure_NumDeals",
  "BuyoutDeals_PrimaryIndustry_IT.Infrastructure_AggValue",
  "BuyoutDeals_PrimaryIndustry_IT.Security.Cybersecurity_NumDeals",
  "BuyoutDeals_PrimaryIndustry_IT.Security.Cybersecurity_AggValue",
  "BuyoutDeals_PrimaryIndustry_Internet_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Internet_AggValue",
  "BuyoutDeals_PrimaryIndustry_Information.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Information.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Telecoms_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Telecoms_AggValue",
  
  # Group 3B: Emerging Tech & Media (Buyout)
  "BuyoutDeals_PrimaryIndustry_Electronics_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Electronics_AggValue",
  "BuyoutDeals_PrimaryIndustry_Media_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Media_AggValue",
  "BuyoutDeals_PrimaryIndustry_Financial.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Financial.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Agribusiness_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Agribusiness_AggValue",
  "BuyoutDeals_PrimaryIndustry_Biotechnology_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Biotechnology_AggValue",
  "BuyoutDeals_PrimaryIndustry_Semiconductors_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Semiconductors_AggValue",
  
  # Group 4: Consumer & Retail (Buyout)
  "BuyoutDeals_PrimaryIndustry_Consumer.Products_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Consumer.Products_AggValue",
  "BuyoutDeals_PrimaryIndustry_Food_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Food_AggValue",
  "BuyoutDeals_PrimaryIndustry_Consumer.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Consumer.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Retail_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Retail_AggValue",
  "BuyoutDeals_PrimaryIndustry_Marketing.Advertising_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Marketing.Advertising_AggValue",
  "BuyoutDeals_PrimaryIndustry_Business.Support.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Business.Support.Services_AggValue",
  
  # Group 5: Industrials & Infrastructure (Buyout)
  "BuyoutDeals_PrimaryIndustry_Aerospace_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Aerospace_AggValue",
  "BuyoutDeals_PrimaryIndustry_Construction_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Construction_AggValue",
  "BuyoutDeals_PrimaryIndustry_Industrial.Machinery_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Industrial.Machinery_AggValue",
  "BuyoutDeals_PrimaryIndustry_Rail.Transport_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Rail.Transport_AggValue",
  "BuyoutDeals_PrimaryIndustry_Ship.Building...Repair_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Ship.Building...Repair_AggValue",
  "BuyoutDeals_PrimaryIndustry_Logistics...Distribution_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Logistics...Distribution_AggValue",
  "BuyoutDeals_PrimaryIndustry_Outsourcing_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Outsourcing_AggValue",
  "BuyoutDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_AggValue",
  "BuyoutDeals_PrimaryIndustry_Commercial.Property_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Commercial.Property_AggValue",
  "BuyoutDeals_PrimaryIndustry_Mining_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Mining_AggValue" ,
  
  
  
  # Group 6: Others / Niche (Buyout)
  "BuyoutDeals_PrimaryIndustry_Defence_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Defence_AggValue",
  "BuyoutDeals_PrimaryIndustry_Environmental.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Environmental.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Hardware_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Hardware_AggValue",
  "BuyoutDeals_PrimaryIndustry_Transportation.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Transportation.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Travel...Leisure_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Travel...Leisure_AggValue",
  "BuyoutDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_AggValue",
  "BuyoutDeals_PrimaryIndustry_Education.Training_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Education.Training_AggValue",
  "BuyoutDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_AggValue",
  "BuyoutDeals_PrimaryIndustry_Bottling_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Bottling_AggValue",
  "BuyoutDeals_PrimaryIndustry_Packaging_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Packaging_AggValue",
  "BuyoutDeals_PrimaryIndustry_Chemicals_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Chemicals_AggValue",
  "BuyoutDeals_PrimaryIndustry_Materials_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Materials_AggValue",
  "BuyoutDeals_PrimaryIndustry_Insurance_NumDeals",
  "BuyoutDeals_PrimaryIndustry_Insurance_AggValue"
)

cols_to_drop_vc <- c(
  # VCDeals Group 1: Tech & Innovation – to be split into two refined groups below.
  "VCDeals_PrimaryIndustry_Software_NumDeals",
  "VCDeals_PrimaryIndustry_Software_AggValue",
  "VCDeals_PrimaryIndustry_Internet_NumDeals",
  "VCDeals_PrimaryIndustry_Internet_AggValue",
  "VCDeals_PrimaryIndustry_Telecoms_NumDeals",
  "VCDeals_PrimaryIndustry_Telecoms_AggValue",
  "VCDeals_PrimaryIndustry_IT.Security.Cybersecurity_NumDeals",
  "VCDeals_PrimaryIndustry_IT.Security.Cybersecurity_AggValue",
  "VCDeals_PrimaryIndustry_IT.Infrastructure_NumDeals",
  "VCDeals_PrimaryIndustry_IT.Infrastructure_AggValue",
  "VCDeals_PrimaryIndustry_Information.Services_NumDeals",
  "VCDeals_PrimaryIndustry_Information.Services_AggValue",
  "VCDeals_PrimaryIndustry_Electronics_NumDeals",
  "VCDeals_PrimaryIndustry_Electronics_AggValue",
  "VCDeals_PrimaryIndustry_Marketing.Advertising_NumDeals",
  "VCDeals_PrimaryIndustry_Marketing.Advertising_AggValue",
  "VCDeals_PrimaryIndustry_Financial.Services_NumDeals",
  "VCDeals_PrimaryIndustry_Financial.Services_AggValue",
  
  # VCDeals Group 2: Healthcare & Life Sciences
  "VCDeals_PrimaryIndustry_Medical.Devices...Equipment_NumDeals",
  "VCDeals_PrimaryIndustry_Medical.Devices...Equipment_AggValue",
  "VCDeals_PrimaryIndustry_Biotechnology_NumDeals",
  "VCDeals_PrimaryIndustry_Biotechnology_AggValue",
  "VCDeals_PrimaryIndustry_Healthcare.IT_NumDeals",
  "VCDeals_PrimaryIndustry_Healthcare.IT_AggValue",
  "VCDeals_PrimaryIndustry_Healthcare_NumDeals",
  "VCDeals_PrimaryIndustry_Healthcare_AggValue",
  "VCDeals_PrimaryIndustry_Healthcare.Specialists_NumDeals",
  "VCDeals_PrimaryIndustry_Healthcare.Specialists_AggValue",
  
  # VCDeals Group 3: Consumer & Retail
  "VCDeals_PrimaryIndustry_Consumer.Products_NumDeals",
  "VCDeals_PrimaryIndustry_Consumer.Products_AggValue",
  "VCDeals_PrimaryIndustry_Consumer.Services_NumDeals",
  "VCDeals_PrimaryIndustry_Consumer.Services_AggValue",
  "VCDeals_PrimaryIndustry_Retail_NumDeals",
  "VCDeals_PrimaryIndustry_Retail_AggValue",
  "VCDeals_PrimaryIndustry_Food_NumDeals",
  "VCDeals_PrimaryIndustry_Food_AggValue",
  
  # VCDeals Group 4: Industrials & Infrastructure
  "VCDeals_PrimaryIndustry_Transportation.Services_NumDeals",
  "VCDeals_PrimaryIndustry_Transportation.Services_AggValue",
  "VCDeals_PrimaryIndustry_Aerospace_NumDeals",
  "VCDeals_PrimaryIndustry_Aerospace_AggValue",
  "VCDeals_PrimaryIndustry_Commercial.Property_NumDeals",
  "VCDeals_PrimaryIndustry_Commercial.Property_AggValue",
  "VCDeals_PrimaryIndustry_Construction_NumDeals",
  "VCDeals_PrimaryIndustry_Construction_AggValue",
  "VCDeals_PrimaryIndustry_Industrial.Machinery_NumDeals",
  "VCDeals_PrimaryIndustry_Industrial.Machinery_AggValue",
  "VCDeals_PrimaryIndustry_Logistics...Distribution_NumDeals",
  "VCDeals_PrimaryIndustry_Logistics...Distribution_AggValue",
  "VCDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_NumDeals",
  "VCDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_AggValue",
  
  # VCDeals Group 5: Energy & Resources
  "VCDeals_PrimaryIndustry_Energy.Storage...Batteries_NumDeals",
  "VCDeals_PrimaryIndustry_Energy.Storage...Batteries_AggValue",
  "VCDeals_PrimaryIndustry_Oil...Gas_NumDeals",
  "VCDeals_PrimaryIndustry_Oil...Gas_AggValue",
  "VCDeals_PrimaryIndustry_Power...Utilities_NumDeals",
  "VCDeals_PrimaryIndustry_Power...Utilities_AggValue",
  "VCDeals_PrimaryIndustry_Renewable.Energy_NumDeals",
  "VCDeals_PrimaryIndustry_Renewable.Energy_AggValue",
  
  # VCDeals Group 6: Others / Niche – will be renamed “Manufacturing”
  "VCDeals_PrimaryIndustry_Hardware_NumDeals",
  "VCDeals_PrimaryIndustry_Hardware_AggValue",
  "VCDeals_PrimaryIndustry_Education.Training_NumDeals",
  "VCDeals_PrimaryIndustry_Education.Training_AggValue",
  "VCDeals_PrimaryIndustry_Travel...Leisure_NumDeals",
  "VCDeals_PrimaryIndustry_Travel...Leisure_AggValue",
  "VCDeals_PrimaryIndustry_Agribusiness_NumDeals",
  "VCDeals_PrimaryIndustry_Agribusiness_AggValue",
  "VCDeals_PrimaryIndustry_Outsourcing_NumDeals",
  "VCDeals_PrimaryIndustry_Outsourcing_AggValue",
  "VCDeals_PrimaryIndustry_Business.Support.Services_NumDeals",
  "VCDeals_PrimaryIndustry_Business.Support.Services_AggValue",
  
  
      "PE_FundTypes_Mezzanine_AggValue",
      "PE_FundTypes_Direct.Lending_AggValue",
      "PE_FundTypes_Distressed.Debt_AggValue",
      "PE_FundTypes_Mezzanine_AggValue",
      "PE_FundTypes_Direct.Lending_AggValue",
      "PE_FundTypes_Distressed.Debt_AggValue",
  
      "PE_FundTypes_Secondaries_NumFunds",
      "PE_FundTypes_Direct.Secondaries_NumFunds",
      "PE_FundTypes_Co.Investment_NumFunds",
      "PE_FundTypes_Co.Investment.Multi.Manager_NumFunds",
      "PE_FundTypes_Secondaries_AggValue",
      "PE_FundTypes_Direct.Secondaries_AggValue",
      "PE_FundTypes_Co.Investment_AggValue",
      "PE_FundTypes_Co.Investment.Multi.Manager_AggValue",
      "VCDeals_PrimaryIndustry_Packaging_NumDeals",
      "VCDeals_PrimaryIndustry_Bottling_NumDeals",
      "VCDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_NumDeals",
      "VCDeals_PrimaryIndustry_Packaging_AggValue",
      "VCDeals_PrimaryIndustry_Bottling_AggValue",
      "VCDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_AggValue",
      "PE_FundTypes_Infrastructure.Opportunistic_NumFunds",
      "PE_FundTypes_Infrastructure.Core_NumFunds",
      "PE_FundTypes_Infrastructure.Value.Added_NumFunds",
      "PE_FundTypes_Infrastructure.Opportunistic_AggValue",
      "PE_FundTypes_Infrastructure.Core_AggValue",
      "PE_FundTypes_Infrastructure.Value.Added_AggValue",
     "VCDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_NumDeals",
  "VCDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_AggValue",
"BuyoutDeals_PrimaryIndustry_Forestry...Timber_NumDeals",             
"BuyoutDeals_PrimaryIndustry_Forestry...Timber_AggValue",
"VCDeals_PrimaryIndustry_Chemicals_NumDeals",                          
"VCDeals_PrimaryIndustry_Chemicals_AggValue",                         
"VCDeals_PrimaryIndustry_Mining_NumDeals",                             
"VCDeals_PrimaryIndustry_Mining_AggValue",
"PE_FundTypes_Direct.Lending_NumFunds",                                
"PE_FundTypes_Distressed.Debt_NumFunds",                               
"PE_FundTypes_Infrastructure.Core.Plus_NumFunds",                      
"PE_FundTypes_Infrastructure.Core.Plus_AggValue",                      
"VCDeals_PrimaryIndustry_Forestry...Timber_NumDeals",                  
"VCDeals_PrimaryIndustry_Forestry...Timber_AggValue",
"PE_FundTypes_Hybrid.Fund.Of.Funds_NumFunds",                          
"PE_FundTypes_Hybrid.Fund.Of.Funds_AggValue",
"PE_FundTypes_Private.Debt.Fund.of.Funds_NumFunds",                    
"PE_FundTypes_Private.Debt.Fund.of.Funds_AggValue",                    
"VCDeals_PrimaryIndustry_Biopolymers_NumDeals",                        
"VCDeals_PrimaryIndustry_Biopolymers_AggValue",                        
"PE_FundTypes_Mezzanine_NumFunds",                                     
"VCDeals_PrimaryIndustry_Ship.Building...Repair_NumDeals",             
"VCDeals_PrimaryIndustry_Ship.Building...Repair_AggValue" 
  
)

cols_to_drop_all <- c(cols_to_drop_buyout, cols_to_drop_vc)

# --- Consolidate Primary Industry Columns for BuyoutDeals and VCDeals ---
preqindata_new <- preqindata_new %>%
  # --- BuyoutDeals Consolidation ---
  mutate(
    BuyoutDeals_PrimaryIndustry_Energy_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Oil...Gas_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Power...Utilities_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Renewable.Energy_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Energy.Storage...Batteries_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Forestry...Timber_NumDeals"
      
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_Energy_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Oil...Gas_AggValue",
      "BuyoutDeals_PrimaryIndustry_Power...Utilities_AggValue",
      "BuyoutDeals_PrimaryIndustry_Renewable.Energy_AggValue",
      "BuyoutDeals_PrimaryIndustry_Energy.Storage...Batteries_AggValue",
      "BuyoutDeals_PrimaryIndustry_Forestry...Timber_AggValue"  
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_Healthcare_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Healthcare_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Healthcare.IT_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Healthcare.Specialists_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Medical.Devices...Equipment_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Pharmaceuticals_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_Healthcare_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Healthcare_AggValue",
      "BuyoutDeals_PrimaryIndustry_Healthcare.IT_AggValue",
      "BuyoutDeals_PrimaryIndustry_Healthcare.Specialists_AggValue",
      "BuyoutDeals_PrimaryIndustry_Medical.Devices...Equipment_AggValue",
      "BuyoutDeals_PrimaryIndustry_Pharmaceuticals_AggValue"
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_DigitalIT_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Software_NumDeals",
      "BuyoutDeals_PrimaryIndustry_IT.Infrastructure_NumDeals",
      "BuyoutDeals_PrimaryIndustry_IT.Security.Cybersecurity_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Internet_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Information.Services_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Telecoms_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_DigitalIT_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Software_AggValue",
      "BuyoutDeals_PrimaryIndustry_IT.Infrastructure_AggValue",
      "BuyoutDeals_PrimaryIndustry_IT.Security.Cybersecurity_AggValue",
      "BuyoutDeals_PrimaryIndustry_Internet_AggValue",
      "BuyoutDeals_PrimaryIndustry_Information.Services_AggValue",
      "BuyoutDeals_PrimaryIndustry_Telecoms_AggValue"
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_EmergingTechMedia_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Electronics_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Media_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Financial.Services_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Agribusiness_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Biotechnology_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Semiconductors_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_EmergingTechMedia_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Electronics_AggValue",
      "BuyoutDeals_PrimaryIndustry_Media_AggValue",
      "BuyoutDeals_PrimaryIndustry_Financial.Services_AggValue",
      "BuyoutDeals_PrimaryIndustry_Agribusiness_AggValue",
      "BuyoutDeals_PrimaryIndustry_Biotechnology_AggValue",
      "BuyoutDeals_PrimaryIndustry_Semiconductors_AggValue"
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_ConsumerRetail_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Consumer.Products_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Food_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Consumer.Services_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Retail_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Marketing.Advertising_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Business.Support.Services_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_ConsumerRetail_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Consumer.Products_AggValue",
      "BuyoutDeals_PrimaryIndustry_Food_AggValue",
      "BuyoutDeals_PrimaryIndustry_Consumer.Services_AggValue",
      "BuyoutDeals_PrimaryIndustry_Retail_AggValue",
      "BuyoutDeals_PrimaryIndustry_Marketing.Advertising_AggValue",
      "BuyoutDeals_PrimaryIndustry_Business.Support.Services_AggValue"
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_Industrials_Infrastructure_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Aerospace_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Construction_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Industrial.Machinery_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Rail.Transport_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Ship.Building...Repair_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Logistics...Distribution_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Outsourcing_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Commercial.Property_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Mining_NumDeals"
      
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_Industrials_Infrastructure_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Aerospace_AggValue",
      "BuyoutDeals_PrimaryIndustry_Construction_AggValue",
      "BuyoutDeals_PrimaryIndustry_Industrial.Machinery_AggValue",
      "BuyoutDeals_PrimaryIndustry_Rail.Transport_AggValue",
      "BuyoutDeals_PrimaryIndustry_Ship.Building...Repair_AggValue",
      "BuyoutDeals_PrimaryIndustry_Logistics...Distribution_AggValue",
      "BuyoutDeals_PrimaryIndustry_Outsourcing_AggValue",
      "BuyoutDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_AggValue",
      "BuyoutDeals_PrimaryIndustry_Commercial.Property_AggValue",
      "BuyoutDeals_PrimaryIndustry_Mining_AggValue"
      
    )), na.rm = TRUE),
    
    BuyoutDeals_PrimaryIndustry_Others_NumDeals = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Defence_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Environmental.Services_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Hardware_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Transportation.Services_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Travel...Leisure_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Education.Training_NumDeals",
      "BuyoutDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_PrimaryIndustry_Others_AggValue = rowSums(across(c(
      "BuyoutDeals_PrimaryIndustry_Defence_AggValue",
      "BuyoutDeals_PrimaryIndustry_Environmental.Services_AggValue",
      "BuyoutDeals_PrimaryIndustry_Hardware_AggValue",
      "BuyoutDeals_PrimaryIndustry_Transportation.Services_AggValue",
      "BuyoutDeals_PrimaryIndustry_Travel...Leisure_AggValue",
      "BuyoutDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_AggValue",
      "BuyoutDeals_PrimaryIndustry_Education.Training_AggValue",
      "BuyoutDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_AggValue"
    )), na.rm = TRUE),
    
    # --- VCDeals Consolidation ---
    # Group 1A: Digital & IT (VC)
    VCDeals_PrimaryIndustry_DigitalIT_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Software_NumDeals",
      "VCDeals_PrimaryIndustry_Internet_NumDeals",
      "VCDeals_PrimaryIndustry_Telecoms_NumDeals",
      "VCDeals_PrimaryIndustry_IT.Security.Cybersecurity_NumDeals",
      "VCDeals_PrimaryIndustry_IT.Infrastructure_NumDeals",
      "VCDeals_PrimaryIndustry_Information.Services_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_DigitalIT_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Software_AggValue",
      "VCDeals_PrimaryIndustry_Internet_AggValue",
      "VCDeals_PrimaryIndustry_Telecoms_AggValue",
      "VCDeals_PrimaryIndustry_IT.Security.Cybersecurity_AggValue",
      "VCDeals_PrimaryIndustry_IT.Infrastructure_AggValue",
      "VCDeals_PrimaryIndustry_Information.Services_AggValue"
    )), na.rm = TRUE),
    
    # Group 1B: Media & Financial (VC)
    VCDeals_PrimaryIndustry_MediaFinancial_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Electronics_NumDeals",
      "VCDeals_PrimaryIndustry_Marketing.Advertising_NumDeals",
      "VCDeals_PrimaryIndustry_Financial.Services_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_MediaFinancial_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Electronics_AggValue",
      "VCDeals_PrimaryIndustry_Marketing.Advertising_AggValue",
      "VCDeals_PrimaryIndustry_Financial.Services_AggValue"
    )), na.rm = TRUE),
    
    # Group 2: Healthcare & Life Sciences (VC)
    VCDeals_PrimaryIndustry_Healthcare_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Medical.Devices...Equipment_NumDeals",
      "VCDeals_PrimaryIndustry_Biotechnology_NumDeals",
      "VCDeals_PrimaryIndustry_Healthcare.IT_NumDeals",
      "VCDeals_PrimaryIndustry_Healthcare_NumDeals",
      "VCDeals_PrimaryIndustry_Healthcare.Specialists_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_Healthcare_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Medical.Devices...Equipment_AggValue",
      "VCDeals_PrimaryIndustry_Biotechnology_AggValue",
      "VCDeals_PrimaryIndustry_Healthcare.IT_AggValue",
      "VCDeals_PrimaryIndustry_Healthcare_AggValue",
      "VCDeals_PrimaryIndustry_Healthcare.Specialists_AggValue"
    )), na.rm = TRUE),
    
    # Group 3: Consumer & Retail (VC)
    VCDeals_PrimaryIndustry_ConsumerRetail_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Consumer.Products_NumDeals",
      "VCDeals_PrimaryIndustry_Consumer.Services_NumDeals",
      "VCDeals_PrimaryIndustry_Retail_NumDeals",
      "VCDeals_PrimaryIndustry_Food_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_ConsumerRetail_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Consumer.Products_AggValue",
      "VCDeals_PrimaryIndustry_Consumer.Services_AggValue",
      "VCDeals_PrimaryIndustry_Retail_AggValue",
      "VCDeals_PrimaryIndustry_Food_AggValue"
    )), na.rm = TRUE),
    
    # Group 4: Industrials & Infrastructure (VC)
    VCDeals_PrimaryIndustry_Industrials_Infrastructure_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Transportation.Services_NumDeals",
      "VCDeals_PrimaryIndustry_Aerospace_NumDeals",
      "VCDeals_PrimaryIndustry_Commercial.Property_NumDeals",
      "VCDeals_PrimaryIndustry_Construction_NumDeals",
      "VCDeals_PrimaryIndustry_Industrial.Machinery_NumDeals",
      "VCDeals_PrimaryIndustry_Logistics...Distribution_NumDeals",
      "VCDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_NumDeals",
      "VCDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_NumDeals",
      "VCDeals_PrimaryIndustry_Mining_NumDeals" ,
      "VCDeals_PrimaryIndustry_Chemicals_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_Industrials_Infrastructure_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Transportation.Services_AggValue",
      "VCDeals_PrimaryIndustry_Aerospace_AggValue",
      "VCDeals_PrimaryIndustry_Commercial.Property_AggValue",
      "VCDeals_PrimaryIndustry_Construction_AggValue",
      "VCDeals_PrimaryIndustry_Industrial.Machinery_AggValue",
      "VCDeals_PrimaryIndustry_Logistics...Distribution_AggValue",
      "VCDeals_PrimaryIndustry_Real.Estate.Development...Operating.Companies_AggValue",
      "VCDeals_PrimaryIndustry_Automobiles..Other.Vehicles...Parts_AggValue",
      "VCDeals_PrimaryIndustry_Chemicals_AggValue",
      "VCDeals_PrimaryIndustry_Mining_AggValue"
    )), na.rm = TRUE),
    
    # Group 5: Energy & Resources (VC)
    VCDeals_PrimaryIndustry_Energy_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Energy.Storage...Batteries_NumDeals",
      "VCDeals_PrimaryIndustry_Power...Utilities_NumDeals",
      "VCDeals_PrimaryIndustry_Renewable.Energy_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_Energy_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Energy.Storage...Batteries_AggValue",
      "VCDeals_PrimaryIndustry_Power...Utilities_AggValue",
      "VCDeals_PrimaryIndustry_Renewable.Energy_AggValue"
    )), na.rm = TRUE),
    
    # Group 6: Others / Niche – rename this group to "Manufacturing" for VCDeals
    VCDeals_PrimaryIndustry_Manufacturing_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Hardware_NumDeals",
      "VCDeals_PrimaryIndustry_Education.Training_NumDeals",
      "VCDeals_PrimaryIndustry_Travel...Leisure_NumDeals",
      "VCDeals_PrimaryIndustry_Agribusiness_NumDeals",
      "VCDeals_PrimaryIndustry_Outsourcing_NumDeals",
      "VCDeals_PrimaryIndustry_Business.Support.Services_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_Manufacturing_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Hardware_AggValue",
      "VCDeals_PrimaryIndustry_Education.Training_AggValue",
      "VCDeals_PrimaryIndustry_Travel...Leisure_AggValue",
      "VCDeals_PrimaryIndustry_Agribusiness_AggValue",
      "VCDeals_PrimaryIndustry_Outsourcing_AggValue",
      "VCDeals_PrimaryIndustry_Business.Support.Services_AggValue"
    )), na.rm = TRUE)
  ) %>%
  # Drop the original detailed primary industry columns for BuyoutDeals and VCDeal
  
  ### Additional Consolidation for Non–Primary Industry Columns ###
  # Update VCDeals Investment Stage (keeping original naming convention)
  mutate(
    VCDeals_InvestmentStage_OtherVenture_NumDeals = rowSums(across(c(
      "VCDeals_InvestmentStage_Unspecified.Round_NumDeals",
      "VCDeals_InvestmentStage_PIPE_NumDeals",
      "VCDeals_InvestmentStage_Merger_NumDeals",
      "VCDeals_InvestmentStage_Venture.Debt_NumDeals",
      "VCDeals_InvestmentStage_Add.on_NumDeals",
      "VCDeals_InvestmentStage_Growth.Capital.Expansion_NumDeals"
    )), na.rm = TRUE),
    VCDeals_InvestmentStage_OtherVenture_AggValue = rowSums(across(c(
      "VCDeals_InvestmentStage_Unspecified.Round_AggValue",
      "VCDeals_InvestmentStage_PIPE_AggValue",
      "VCDeals_InvestmentStage_Merger_AggValue",
      "VCDeals_InvestmentStage_Venture.Debt_AggValue",
      "VCDeals_InvestmentStage_Add.on_AggValue",
      "VCDeals_InvestmentStage_Growth.Capital.Expansion_AggValue"
    )), na.rm = TRUE)
  ) %>%
  # Drop original VCDeals Investment Stage columns (except those now aggregated)
  select(-c(
    "VCDeals_InvestmentStage_Unspecified.Round_NumDeals",
    "VCDeals_InvestmentStage_Unspecified.Round_AggValue",
    "VCDeals_InvestmentStage_PIPE_NumDeals",
    "VCDeals_InvestmentStage_PIPE_AggValue",
    "VCDeals_InvestmentStage_Merger_NumDeals",
    "VCDeals_InvestmentStage_Merger_AggValue",
    "VCDeals_InvestmentStage_Venture.Debt_NumDeals",
    "VCDeals_InvestmentStage_Venture.Debt_AggValue",
    "VCDeals_InvestmentStage_Add.on_NumDeals",
    "VCDeals_InvestmentStage_Add.on_AggValue"
  )) %>%
  
  # Consolidate BuyoutDeals Investment Type into "BuyoutDeals_PrimaryIndustry_InvestmentType_Other"
  mutate(
    BuyoutDeals_InvestmentType_Other_NumDeals = rowSums(across(c(
      "BuyoutDeals_InvestmentType_Add.on_NumDeals",
      "BuyoutDeals_InvestmentType_Public.To.Private_NumDeals",
      "BuyoutDeals_InvestmentType_PIPE_NumDeals",
      "BuyoutDeals_InvestmentType_Recapitalisation_NumDeals",
      "BuyoutDeals_InvestmentType_Merger_NumDeals",
      "BuyoutDeals_InvestmentType_Restructuring_NumDeals"
    )), na.rm = TRUE),
    BuyoutDeals_InvestmentType_Other_AggValue = rowSums(across(c(
      "BuyoutDeals_InvestmentType_Add.on_AggValue",
      "BuyoutDeals_InvestmentType_Public.To.Private_AggValue",
      "BuyoutDeals_InvestmentType_PIPE_AggValue",
      "BuyoutDeals_InvestmentType_Recapitalisation_AggValue",
      "BuyoutDeals_InvestmentType_Merger_AggValue",
      "BuyoutDeals_InvestmentType_Restructuring_AggValue"
    )), na.rm = TRUE)
  ) %>%
  select(-c(
    "BuyoutDeals_InvestmentType_Add.on_NumDeals",
    "BuyoutDeals_InvestmentType_Add.on_AggValue",
    "BuyoutDeals_InvestmentType_Public.To.Private_NumDeals",
    "BuyoutDeals_InvestmentType_Public.To.Private_AggValue",
    "BuyoutDeals_InvestmentType_PIPE_NumDeals",
    "BuyoutDeals_InvestmentType_PIPE_AggValue",
    "BuyoutDeals_InvestmentType_Recapitalisation_NumDeals",
    "BuyoutDeals_InvestmentType_Recapitalisation_AggValue",
    "BuyoutDeals_InvestmentType_Merger_NumDeals",
    "BuyoutDeals_InvestmentType_Merger_AggValue",
    "BuyoutDeals_InvestmentType_Restructuring_NumDeals",
    "BuyoutDeals_InvestmentType_Restructuring_AggValue"
  )) %>%
  
  # Consolidate PE Fund Types into "PE_FundTypes_Venture_Early"
  mutate(
    PE_FundTypes_Venture_Early_NumFunds = rowSums(across(c(
      "PE_FundTypes_Venture..General._NumFunds",
      "PE_FundTypes_Early.Stage_NumFunds",
      "PE_FundTypes_Early.Stage..Seed_NumFunds"
    )), na.rm = TRUE),
    PE_FundTypes_Venture_Early_AggValue = rowSums(across(c(
      "PE_FundTypes_Venture..General._AggValue",
      "PE_FundTypes_Early.Stage_AggValue",
      "PE_FundTypes_Early.Stage..Seed_AggValue"
    )), na.rm = TRUE)
  ) %>%
  select(-c(
    "PE_FundTypes_Venture..General._NumFunds",
    "PE_FundTypes_Venture..General._AggValue",
    "PE_FundTypes_Early.Stage_NumFunds",
    "PE_FundTypes_Early.Stage_AggValue",
    "PE_FundTypes_Early.Stage..Seed_NumFunds",
    "PE_FundTypes_Early.Stage..Seed_AggValue"
  )) %>%
  
  # Additional Non–Primary Industry Consolidations
  mutate(
    # Consolidate PE Secondaries & Co-Investment into "PE_FundTypes_Secondaries_Broader"
    PE_FundTypes_Secondaries_NumFunds = rowSums(across(c(
      "PE_FundTypes_Secondaries_NumFunds",
      "PE_FundTypes_Direct.Secondaries_NumFunds",
      "PE_FundTypes_Co.Investment_NumFunds",
      "PE_FundTypes_Co.Investment.Multi.Manager_NumFunds"
    )), na.rm = TRUE),
    PE_FundTypes_Secondaries_AggValue = rowSums(across(c(
      "PE_FundTypes_Secondaries_AggValue",
      "PE_FundTypes_Direct.Secondaries_AggValue",
      "PE_FundTypes_Co.Investment_AggValue",
      "PE_FundTypes_Co.Investment.Multi.Manager_AggValue"
    )), na.rm = TRUE)
  ) %>%
  mutate(
    # Consolidate PE Infrastructure Fund Types into "PE_FundTypes_Infrastructure_FundTypes"
    PE_FundTypes_Infrastructure_NumFunds = rowSums(across(c(
      "PE_FundTypes_Infrastructure.Opportunistic_NumFunds",
      "PE_FundTypes_Infrastructure.Core_NumFunds",
      "PE_FundTypes_Infrastructure.Value.Added_NumFunds"
    )), na.rm = TRUE),
    PE_FundTypes_Infrastructure_AggValue = rowSums(across(c(
      "PE_FundTypes_Infrastructure.Opportunistic_AggValue",
      "PE_FundTypes_Infrastructure.Core_AggValue",
      "PE_FundTypes_Infrastructure.Value.Added_AggValue"
    )), na.rm = TRUE)
  ) %>%
  mutate(
    # Consolidate Fund-of-Funds into "PE_FundTypes_PrivateCredit_FoF"
    PE_FundTypes_FoF_NumFunds = rowSums(across(c(
      "PE_FundTypes_Hybrid.Fund.Of.Funds_NumFunds",
      "PE_FundTypes_Fund.of.Funds_NumFunds",
      "PE_FundTypes_Private.Debt.Fund.of.Funds_NumFunds"
    )), na.rm = TRUE),
    PE_FundTypes_FoF_AggValue = rowSums(across(c(
      "PE_FundTypes_Hybrid.Fund.Of.Funds_AggValue",
      "PE_FundTypes_Fund.of.Funds_AggValue",
      "PE_FundTypes_Private.Debt.Fund.of.Funds_AggValue"
    )), na.rm = TRUE)
  ) %>%
  mutate(
    # Consolidate Private Credit details into "PE_FundTypes_PrivateCredit"
    PE_FundTypes_PrivateCredit_NumFunds = rowSums(across(c(
      "PE_FundTypes_Mezzanine_NumFunds",
      "PE_FundTypes_Direct.Lending_NumFunds",
      "PE_FundTypes_Distressed.Debt_NumFunds"
    )), na.rm = TRUE),
    PE_FundTypes_PrivateCredit_AggValue = rowSums(across(c(
      "PE_FundTypes_Mezzanine_AggValue",
      "PE_FundTypes_Direct.Lending_AggValue",
      "PE_FundTypes_Distressed.Debt_AggValue"
    )), na.rm = TRUE)
  ) %>%
  mutate(
    # Consolidate VCDeals Manufacturing (Packaging + Bottling + Heating...Ventilation) 
    VCDeals_PrimaryIndustry_Manufacturing_NumDeals = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Packaging_NumDeals",
      "VCDeals_PrimaryIndustry_Bottling_NumDeals",
      "VCDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_NumDeals"
    )), na.rm = TRUE),
    VCDeals_PrimaryIndustry_Manufacturing_AggValue = rowSums(across(c(
      "VCDeals_PrimaryIndustry_Packaging_AggValue",
      "VCDeals_PrimaryIndustry_Bottling_AggValue",
      "VCDeals_PrimaryIndustry_Heating..Cooling...Ventilation.Equipment.and.Services_AggValue"
    )), na.rm = TRUE)
  ) %>% 
  select(-all_of(cols_to_drop_all))
colnames(preqindata_new)


# write.csv(preqindata_new, "LP_data_cleaned.csv")

```

# See how many values are missing committed capital values
```{r}
# Assume preqindata_new is your data frame

# Identify all columns with the pattern "_NumFunds"
num_funds_columns <- grep("_NumFunds$", names(preqindata_new), value = TRUE)

# Create the corresponding aggregated value column names by replacing the suffix
agg_value_columns <- sub("_NumFunds$", "_AggValue", num_funds_columns)

# Initialize an empty list to store the results for each pair
results_list <- list()

# Loop over each pair of columns
for (i in seq_along(num_funds_columns)) {
  num_col <- num_funds_columns[i]
  agg_col <- agg_value_columns[i]
  
  # Check if the aggregated column exists in the dataset
  if (agg_col %in% names(preqindata_new)) {
    # Find rows where the number of funds is greater than 0 but the aggregated value is 0
    condition <- preqindata_new[[num_col]] > 0 & preqindata_new[[agg_col]] == 0
    
    # If there are any such cases, subset the data and record the pair name
    if (any(condition, na.rm = TRUE)) {
      subset_data <- preqindata_new[condition, ]
      subset_data$VariablePair <- num_col  # optional: indicate which variable pair is in use
      results_list[[num_col]] <- subset_data
    }
  }
}

# Combine all the results into one data frame (if any cases were found)
if (length(results_list) > 0) {
  result <- do.call(rbind, results_list)
  print(result)
} else {
  print("No cases found where numfunds > 0 and the corresponding agg value equals 0.")
}

## rename FundType column to LPType

```


```{r}
preqindata_new <- preqindata_new %>%
  rename(LPType = FundType)


preqindata_new <- preqindata_new %>% 
  rowwise() %>% 
  mutate(
    # Extract the sentence that contains any of the keywords and also a number
    extracted_sentence = {
      sentences <- str_split(PE_Preferences, "\\.\\s*")[[1]]
      # Define a keyword pattern (case-insensitive)
      keyword_pattern <- regex("invests|commits|ticket size", ignore_case = TRUE)
      # Look for sentences that have at least one digit and the keywords
      matching_sentences <- sentences[
        str_detect(sentences, keyword_pattern) & str_detect(sentences, "\\d+")
      ]
      if(length(matching_sentences) > 0) matching_sentences[1] else NA_character_
    },
    # Extract numbers with units and convert to a numeric value in millions
    ticket_size_millions = {
      if(!is.na(extracted_sentence)) {
        # Regex to capture numbers (with optional decimals) and a unit (million, billion, or thousand)
        matches <- str_match_all(extracted_sentence, "(\\d+(\\.\\d+)?)\\s*(million|billion|thousand)")[[1]]
        if(nrow(matches) > 0) {
          # Extract the numeric parts and convert to numbers
          numbers <- as.numeric(matches[,2])
          # Use the first matched unit (assumed consistent)
          unit <- tolower(matches[1,4])
          # If more than one number is found, average the first and last number
          value <- if(length(numbers) >= 2) mean(c(numbers[1], numbers[length(numbers)])) else numbers[1]
          # Convert the value to millions based on the unit
          if(unit == "million") {
            value_converted <- value
          } else if(unit == "billion") {
            value_converted <- value * 1000
          } else if(unit == "thousand") {
            value_converted <- value / 1000
          } else {
            value_converted <- NA_real_
          }
          value_converted
        } else {
          NA_real_
        }
      } else {
        NA_real_
      }
    }
  ) %>% 
  ungroup()


preqindata_new <- preqindata_new %>% 
  mutate(ticket_size_millions = case_when(
    LP_Name == "Nassau Reinsurance" ~ 15,
    LP_Name == "Wisconsin" ~ 17.5
  ))

preqindata_new <- preqindata_new %>%
  mutate(
    AUM_line = as.numeric(gsub("[^0-9.]", "", AUM_line))
  )

preqindata_new <- preqindata_new %>%
  mutate(AUM_line = case_when(
    LP_Name == "American General Life" ~ 161000,
    LP_Name == "CPP" ~ 594448,
    LP_Name == "Merseyside Pension Fund" ~ 13708,
    LP_Name == "Royal Borough London" ~ 1376,
    LP_Name == "CDPQ" ~ 368428,
    LP_Name == "Manulife Financial" ~ 295834,
    LP_Name == "Clal Insurance" ~ 97307,
    LP_Name == "Korea National Pension Service" ~ 811460,
    TRUE ~ AUM_line  # Retain original value for all other LPs
  ))

preqindata_new <- preqindata_new %>%
  mutate(PE_Snapshot_Current_MN = case_when(
    LP_Name == "Duke" ~ .15 * AUM_line,
    LP_Name == "Texas A&M" ~ .26 * AUM_line,
    LP_Name == "Nassau Reinsurance" ~ ticket_size_millions * PE_Commitments_NumberOfFunds,
    LP_Name == "Wisconsin" ~ ticket_size_millions * PE_Commitments_NumberOfFunds,
    LP_Name == "Northwestern Memorial Healthcare" ~ ticket_size_millions * PE_Commitments_NumberOfFunds,
    TRUE ~ PE_Snapshot_Current_MN
  ))

preqindata_new <- preqindata_new %>%
  mutate(PE_Commitments_NumberOfFunds = case_when(
    LP_Name == "Liberty Mutual" ~ 218,
    LP_Name == "State of Hawaii Retirement" ~ 308,
    LP_Name == "Pepperdine" ~ 35,
    LP_Name == "Oregon State" ~ 31,
    LP_Name == "Mount Holyoke" ~ 37,
    LP_Name == "Pomona" ~ 43,
    LP_Name == "U Louisville" ~ 59,
    LP_Name == "Macalester College" ~ 24,
    LP_Name == "Travelers Companies" ~ 356,
    LP_Name == "SMU" ~ 50,
    TRUE ~ PE_Commitments_NumberOfFunds
  ))

median_ratio <- median(preqindata_new$ticket_size_millions / preqindata_new$PE_Snapshot_Current_MN, na.rm = TRUE)

# Impute missing ticket_size_millions based on the median ratio and committed capital
preqindata_new <- preqindata_new %>%
  mutate(ticket_size_millions = ifelse(is.na(ticket_size_millions),
                                        PE_Snapshot_Current_MN * median_ratio,
                                        ticket_size_millions))


df<- df %>% left_join(select(preqindata_new, LP_Name, ticket_size_millions), by = c("LP" = "LP_Name"))

```
```{r}
# Define a helper function to compute a weighted mean that returns NA if all values are missing
library(dplyr)

# First, convert the performance metric columns to numeric.
df <- df %>%
  mutate(
    `NET.IRR....` = as.numeric(`NET.IRR....`),
    `NET.MULTIPLE..X.` = as.numeric(`NET.MULTIPLE..X.`),
    `DPI....` = as.numeric(`DPI....`),
    `RVPI....` = as.numeric(`RVPI....`),
    `MEDIAN.BENCHMARK.NET.IRR....` = as.numeric(`MEDIAN.BENCHMARK.NET.IRR....`),
    `MEDIAN BENCHMARK NET MULTIPLE (X)` = as.numeric(`MEDIAN BENCHMARK NET MULTIPLE (X)`)
  )

# Define a helper function that ensures x and w are numeric and returns NA if no valid weighted mean can be computed.
safe_weighted_mean <- function(x, w) {
  x <- as.numeric(x)
  w <- as.numeric(w)
  out <- weighted.mean(x, w, na.rm = TRUE)
  if (is.nan(out)) NA_real_ else out
}

# Now compute portfolio-level performance, grouping by LP.
portfolio_perf <- df %>%
  group_by(LP) %>%  # Group by LP; each LP has multiple funds.
  summarise(
    total_ticket = sum(ticket_size_millions, na.rm = TRUE),
    
    # Compute weighted averages for performance metrics.
    portfolio_IRR = safe_weighted_mean(`NET.IRR....`, ticket_size_millions),
    portfolio_multiple = safe_weighted_mean(`NET.MULTIPLE..X.`, ticket_size_millions),
    portfolio_DPI = safe_weighted_mean(`DPI....`, ticket_size_millions),
    portfolio_RVPI = safe_weighted_mean(`RVPI....`, ticket_size_millions),
    
    # Composite metric: Total Value Multiple (TVM) = DPI + RVPI; if either is NA, result is NA.
    portfolio_TVM = ifelse(is.na(portfolio_DPI) | is.na(portfolio_RVPI),
                           NA_real_,
                           portfolio_DPI + portfolio_RVPI),
    
    # Weighted averages for benchmark metrics.
    portfolio_bench_IRR = safe_weighted_mean(`MEDIAN.BENCHMARK.NET.IRR....`, ticket_size_millions),
    portfolio_bench_multiple = safe_weighted_mean(`MEDIAN BENCHMARK NET MULTIPLE (X)`, ticket_size_millions),
    
    # Excess performance versus benchmarks.
    excess_IRR = ifelse(is.na(portfolio_IRR) | is.na(portfolio_bench_IRR),
                        NA_real_,
                        portfolio_IRR - portfolio_bench_IRR),
    excess_multiple = ifelse(is.na(portfolio_multiple) | is.na(portfolio_bench_multiple),
                             NA_real_,
                             portfolio_multiple - portfolio_bench_multiple)
  ) %>%
  ungroup()


```
```{r}
library(dplyr)

# Helper function from your snippet
safe_weighted_mean <- function(x, w) {
  x <- as.numeric(x)
  w <- as.numeric(w)
  out <- weighted.mean(x, w, na.rm = TRUE)
  if (is.nan(out)) NA_real_ else out
}

preqindata_new %>% select(LPType, LP_Name)
preqindata_new <- preqindata_new %>% mutate(LPType = case_when(
  LPType %in% c("Insurance Company Canada", "Insurance Company Israel") ~ "Insurance Company",
  LPType %in% c("Public Pension Fund Canada", "Public Pension Fund South Korea") ~ "Public Pension Fund",
  LP_Name == "American General Life" ~ "Insurance Company",
  TRUE ~ LPType
))

```


```{r}
df <- df %>% left_join(select(preqindata_new, LPType, LP_Name, AUM_line), by = c("LP" = "LP_Name"))
table(df$LPType)
# 1. Weighted IRR at the LP–Vintage level
df_lp_vintage <- df %>%
  mutate(
    NET.IRR.... = as.numeric(NET.IRR....),
    ticket_size_millions = as.numeric(ticket_size_millions)
  ) %>%
  group_by(LP, LPType, VINTAGE = `VINTAGE...INCEPTION.YEAR`) %>%
  summarise(
    # Weighted IRR among the funds from the same vintage, for each LP
    vintage_weighted_IRR = safe_weighted_mean(NET.IRR...., ticket_size_millions),
    .groups = "drop"
  )

df_vintage_summary <- df_lp_vintage %>%
  group_by(LPType, VINTAGE) %>%
  summarise(
    median_IRR = median(vintage_weighted_IRR, na.rm = TRUE),
    min_IRR = min(vintage_weighted_IRR, na.rm = TRUE),
    max_IRR = max(vintage_weighted_IRR, na.rm = TRUE),
    .groups = "drop"
  )


# Filter the summary data to include only vintages 2020 or later
df_vintage_summary_filtered <- df_vintage_summary %>% 
  filter(VINTAGE <= 2020 & VINTAGE >= 1990)

# Plot the filtered data
p <- ggplot(df_vintage_summary_filtered, aes(x = VINTAGE, y = median_IRR, group = LPType)) +
  geom_ribbon(
    aes(ymin = min_IRR, ymax = max_IRR, fill = LPType),
    alpha = 0.2
  ) +
  geom_line(aes(color = LPType), size = 1) +
  facet_wrap(~ LPType) +
  labs(
    title = "Fund-Level Performance by LP Type (1990 - 2020)",
    x = "Vintage Year",
    y = "Weighted Median IRR (%)",
    fill = "Confidence Interval\n(min, max)",
    color = "LP Type"
  ) +
  theme_minimal() +
  theme(
    # Increase spacing around the plot if needed
    plot.margin = margin(1, 1, 1, 1, "cm")
  )

# Print the plot
print(p)

# Save the plot with wider dimensions (12x8 inches)
ggsave("vintage_performance.png", plot = p, width = 12, height = 8, dpi = 300)

p_overlay <- ggplot(df_vintage_summary_filtered, aes(x = VINTAGE, y = median_IRR, color = LPType)) +
  geom_line(size = 1) +
  labs(
    title = "Overlay of Fund-Level Performance (Vintages >= 2020) by LP Type",
    x = "Vintage Year",
    y = "Weighted Median IRR (%)",
    color = "LP Type"
  ) +
  theme_minimal()

# Print the plot
print(p_overlay)

p_overlay_with_ci <- ggplot(df_vintage_summary_filtered, aes(x = VINTAGE, y = median_IRR, group = LPType)) +
  geom_ribbon(aes(ymin = min_IRR, ymax = max_IRR, fill = LPType), alpha = 0.2) +
  geom_line(aes(color = LPType), size = 1) +
  labs(
    title = "Private Equity Performance by LP Type (1990-2020)",
    x = "Vintage Year",
    y = "Weighted Median IRR (%)",
    fill = "Confidence Interval\n(min, max)",
    color = "LP Type"
  ) +
  theme_minimal()

# Print the plot
print(p_overlay_with_ci)

```
```{r}
# Compute median IRR by Asset Class and Vintage
df_asset_vintage <- df %>%
  group_by(`ASSET CLASS`, VINTAGE = `VINTAGE...INCEPTION.YEAR`) %>%
  summarise(median_IRR = median(`NET.IRR....`, na.rm = TRUE),
            .groups = "drop") %>%
  filter(`ASSET CLASS` %in% c("Private Equity", "Venture Capital"),VINTAGE <= 2020)

# Plot the median IRR over time, colored by Asset Class
p_asset_line <- ggplot(df_asset_vintage, aes(x = VINTAGE, y = median_IRR, color = `ASSET CLASS`)) +
  geom_line(size = 1) +
  labs(
    title = "Median IRR Over Vintages by Asset Class",
    x = "Vintage Year",
    y = "Median IRR (%)",
    color = "Asset Class"
  ) +
  theme_minimal()
print(p_asset_line)
```


```{r}

table(df$PE..PRIMARY.STRATEGY)
# Identify the top 3 primary strategies by count


# Filter df for only these top strategies
df_top <- df %>% 
  filter(`PE..PRIMARY.STRATEGY` %in% c("Buyout", "Growth", "Secondaries", "Fund of Funds"))

# Compute median IRR for each primary strategy and vintage year
df_strategy_vintage <- df_top %>%
  mutate(`NET.IRR....` = as.numeric(`NET.IRR....`)) %>%  # ensure IRR is numeric
  group_by(`PE..PRIMARY.STRATEGY`, VINTAGE = `VINTAGE...INCEPTION.YEAR`) %>%
  summarise(
    median_IRR = median(`NET.IRR....`, na.rm = TRUE),
    .groups = "drop"
  )

# Plot the median IRR over vintages for the top 3 strategies
p_strategy <- ggplot(df_strategy_vintage, aes(x = VINTAGE, y = median_IRR, color = `PE..PRIMARY.STRATEGY`)) +
  geom_line(size = 1) +
  labs(
    title = "Median IRR Over Vintages Based on Primary Strategies",
    x = "Vintage Year",
    y = "Median IRR (%)",
    color = "Primary Strategy"
  ) +
  theme_minimal()

print(p_strategy)

```

```{r}


# --- Step 0: Define the LP subset ---

# Sample 5 LPs from each LP type from preqindata_new.
# Ensure preqindata_new has columns: LP_Name, AUM_line, LPType.
subset_LPs <- preqindata_new %>%
  group_by(LPType) %>%
  sample_n(5) %>%
  ungroup() %>%
  pull(LP_Name)

# --- Step 1: Create LP-fund relationships using FUND.ID ---
lp_funds <- df %>% 
  select(LP, Fund = `FUND.ID`) %>% 
  distinct()

# Filter to only include LPs in the subset.
lp_funds <- lp_funds %>% 
  filter(LP %in% subset_LPs)

# --- Step 2: Create a list of funds for each LP ---
lp_funds_list <- lp_funds %>%
  group_by(LP) %>%
  summarise(Funds = list(unique(Fund)), .groups = "drop")

# --- Step 3: Generate all unique LP pairs and compute shared funds ---
lp_pairs <- lp_funds_list %>%
  rename(LP1 = LP, Funds1 = Funds) %>%
  inner_join(lp_funds_list %>% rename(LP2 = LP, Funds2 = Funds),
             by = character()) %>%
  # Ensure unique pairs: exclude self-pairs and keep only one ordering (LP1 < LP2)
  filter(LP1 < LP2) %>%
  mutate(shared = map2_int(Funds1, Funds2, ~ length(intersect(.x, .y)))) %>%
  filter(shared > 0) %>%
  select(LP1, LP2, shared)

# --- Step 4: Create the network graph ---
g_lp <- graph_from_data_frame(lp_pairs, directed = FALSE)

# --- Step 5: Attach LP-level attributes from preqindata_new ---
# Preqindata_new should have: LP_Name, AUM_line, LPType.
lp_attr <- preqindata_new %>% 
  filter(LP_Name %in% subset_LPs) %>%
  select(LP_Name, AUM_line, LPType)

V(g_lp)$AUM_line <- lp_attr$AUM_line[match(V(g_lp)$name, lp_attr$LP_Name)]
V(g_lp)$LPType <- lp_attr$LPType[match(V(g_lp)$name, lp_attr$LP_Name)]

# --- Step 6: Assign colors based on LPType ---
V(g_lp)$color <- ifelse(V(g_lp)$LPType == "Insurance Company", "red",
                        ifelse(V(g_lp)$LPType == "Public Pension Fund", "blue", "green"))

# --- Step 7: Define a scaling factor for node sizes ---
scaling_factor <- 10000 # Adjust as needed

# --- Step 8: Plot the network ---
# If you want to specify weights in the layout, you can do so:
library(dplyr)
library(igraph)

# (Assuming g_lp is already created as in previous steps.)

# Use Kamada-Kawai layout with weights (so that higher shared funds pull nodes together)
layout_kk <- layout_with_kk(g_lp, weights = E(g_lp)$shared)

# Define scaling factors
node_scaling_factor <- 1/10  # Adjust as needed; we're using sqrt transformation for AUM
edge_scaling_factor <- 15    # Adjust to reduce edge thickness

# Open a PNG device for a larger plot
png("lp_network_large.png", width = 1600, height = 1200, res = 150)

# Update vertex colors: 
# Use "red" for Insurance Company, "skyblue" for Public Pension Fund (lighter blue), and "green" for others.
V(g_lp)$color <- ifelse(V(g_lp)$LPType == "Insurance Company", "red",
                        ifelse(V(g_lp)$LPType == "Public Pension Fund", "skyblue", "green"))

# Plot the network graph with the Kamada-Kawai layout.
plot(g_lp,
     layout = layout_kk,
     vertex.size = as.numeric(sqrt(V(g_lp)$AUM_line + 1)) * node_scaling_factor,
     vertex.label = V(g_lp)$name,
     vertex.label.cex = 0.8,         # adjust text size
     vertex.label.color = "black",   # change label color
     vertex.label.font = 2,          # bold labels
     vertex.label.family = "sans",   # use sans-serif font
     vertex.label.dist = 0.5,        # distance from the node
     vertex.label.degree = 0,        # angle (0 = right side)
     edge.width = E(g_lp)$shared / edge_scaling_factor,
     main = "Interconnectedness of LP Subset"
)

# Add a legend for node colors (LPType)
 graphics::legend("topleft",
      legend = c("Insurance Company", "Public Pension Fund", "Endowment"),
       col = c("red", "skyblue", "green"),
       pch = 16, pt.cex = 1.5,
       title = "LP Type",
       bty = "n")

dev.off()

```

## processing text data

```{r}
# Load required libraries


# Define a vector of consultant names (all in lowercase for matching)
consultants <- c("cambridge_associates", 
                 "callan associates", 
                 "aon investments", 
                 "wilshire associates", 
                 "nepc", 
                 "stepstone group", 
                 "blackrock investment management", 
                 "aksia", 
                 "meketa investment group", 
                 "cliffwater")

# Create dummy variables for each consultant
for (cons in consultants) {
  # Create a safe column name by replacing spaces with underscores
  dummy_name <- paste0("consultant_", str_replace_all(cons, "\\s+", "_"))
  # Create the dummy: 1 if the consultant name is found in PE_Preferences, 0 otherwise.
  preqindata_new[[dummy_name]] <- ifelse(str_detect(tolower(preqindata_new$PE_Summary), cons), 1, 0)
}

# Create an in-house dummy variable:
# It equals 1 when none of the consultant names are mentioned in PE_Preferences.
# We build a regex pattern that matches any of the consultant names.
regex_pattern <- paste(consultants, collapse = "|")
preqindata_new$in_house <- ifelse(!str_detect(tolower(preqindata_new$PE_Summary), regex_pattern), 1, 0)

# remove GPs with NA value
df <- df %>%
  filter(!is.na(`FUND MANAGER`))


# Define a helper function to compute quartile bins
create_close_bin <- function(x) {
  # Compute quantile breaks for x
  q <- quantile(x, probs = seq(0, 1, 0.25), na.rm = TRUE)
  # If there aren't at least 2 unique breaks, all values are the same.
  if(length(unique(q)) < 2) {
    # Assign a single bin label to all observations
    return(factor(rep("All", length(x))))
  } else {
    # Otherwise, use the unique quantile breaks to create bins
    return(cut(x, breaks = unique(q), include.lowest = TRUE))
  }
}

df <- df %>%
  group_by(`ASSET CLASS`, `VINTAGE...INCEPTION.YEAR`) %>%
  mutate(close_bin = create_close_bin(`FINAL CLOSE SIZE (USD MN)`)) %>%
  ungroup()
df$close_bin

sum(is.na(df$`FUND MANAGER TOTAL AUM (USD MN)`))
# Step 2: Group by FUND MANAGER, ASSET CLASS, VINTAGE / INCEPTION YEAR, and close_bin
# Then impute missing FUND MANAGER TOTAL AUM using the median value from that group.
df <- df %>%
  group_by(`ASSET CLASS`, `VINTAGE...INCEPTION.YEAR`, close_bin) %>%
  mutate(`FUND MANAGER TOTAL AUM (USD MN)` = ifelse(is.na(`FUND MANAGER TOTAL AUM (USD MN)`),
                                                    median(`FUND MANAGER TOTAL AUM (USD MN)`, na.rm = TRUE),
                                                    `FUND MANAGER TOTAL AUM (USD MN)`)) %>%
  ungroup()

gp_modeling <- df

preqindata_new <- preqindata_new %>%
  mutate(InvestorProfile = if_else(InvestorProfile == "et\nMemorial Sloan-Kettering Cancer Center",
                                    "Memorial Sloan-Kettering Cancer Center",
                                    InvestorProfile))


lp_modeling <- preqindata_new %>%
  select(
    InvestorProfile,
    LPType,
    LP_Name,
    AUM_line, 
    TeamSize, 
    PE_Snapshot_Current_MN, 
    PE_Commitments_NumberOfFunds, 
    ticket_size_millions,
    consultant_cambridge_associates,
    consultant_callan_associates,
    consultant_aon_investments,
    consultant_wilshire_associates,
    consultant_nepc,
    consultant_stepstone_group,
    consultant_blackrock_investment_management,
    consultant_aksia,
    consultant_meketa_investment_group,
    consultant_cliffwater,
    in_house
  )



library(stringr)

# Define a function to clean column names
clean_column_names <- function(names_vector) {
  # Convert to lowercase
  names_vector <- tolower(names_vector)
  
  # Trim any leading/trailing white space
  names_vector <- str_trim(names_vector)
  
  # Remove parentheses (but keep the text inside)
  names_vector <- gsub("[()]", "", names_vector)
  
  # Remove unwanted symbols (keep letters, digits, space, and the % sign)
  # This replaces any character that is NOT a letter, digit, space, or % with a space.
  names_vector <- gsub("[^a-z0-9 %]+", " ", names_vector)
  
  # Replace multiple spaces with a single space
  names_vector <- str_squish(names_vector)
  
  # Replace spaces with underscores
  names_vector <- gsub(" ", "_", names_vector)
  
  return(names_vector)
}

# For example, if your dataframe is named 'df', clean its column names:
names(gp_modeling) <- clean_column_names(names(gp_modeling))

# To view the cleaned names:
final_modeling <- gp_modeling %>%
  left_join(lp_modeling, by = c("lp" = "LP_Name"))
final_modeling <- final_modeling %>%
  mutate(period = floor(vintage_inception_year /2 ) * 2)
```


```{r}
colnames(final_modeling)
fund_agg <- final_modeling %>%
  group_by(lp, period) %>%
  summarise(
    total_ticket = sum(ticket_size_millions.x, na.rm = TRUE),
    weighted_net_irr = weighted.mean(net_irr, ticket_size_millions.x, na.rm = TRUE),
    .groups = "drop"
  )

final_modeling <- final_modeling %>%
  mutate(strategy = case_when(
    strategy %in% c("Growth", "Expansion / Late Stage") ~ "Growth Equity",
    strategy %in% c("Early Stage", "Early Stage: Start-up", "Early Stage: Seed") ~ "Early Stage Venture",
    strategy == "Private Debt Fund of Funds" ~ "Fund of Funds",
    strategy == "Co-Investment Multi-Manager" ~ "Co-Investment",
    strategy %in% c("Special Situations", "Turnaround") ~ "Buyout",
    strategy %in% c("Direct Secondaries") ~ "Secondaries",
    strategy %in% c("Infrastructure Core", 
                    "Infrastructure Core Plus", 
                    "Infrastructure Opportunistic", 
                    "Infrastructure Value Added", 
                    "Natural Resources",
                    "Distressed Debt", 
                    "Direct Lending - Junior / Subordinated Debt", 
                    "Direct Lending") ~ "Other",
    TRUE ~ strategy
  ))
library(dplyr)

final_modeling <- final_modeling %>%
  mutate(geographic_focus = case_when(
    geographic_focus %in% c("US", "North America", "Canada", "Midwest", "Northeast") ~ "North America",
    geographic_focus %in% c("China", "Asia", "Japan", "South Korea", "India", "Thailand", "Malaysia", "Vietnam", "East and Southeast Asia", "Greater China", "Taiwan - China") ~ "Asia",
    geographic_focus %in% c("Australasia", "Australia") ~ "Australasia",
    geographic_focus %in% c("Europe", "UK", "Nordic", "West", "West Europe", "France", "Germany", "Central and East Europe", "Netherlands", "Ireland", "Spain") ~ "Europe",
    geographic_focus %in% c("Brazil", "South America", "Americas", "Mexico", "British Virgin Islands") ~ "Americas",
    geographic_focus %in% c("Middle East", "Israel", "Turkey") ~ "Middle East & Israel",
    geographic_focus %in% c("Africa", "North Africa", "Sub-Saharan Africa", "South Africa", "Morocco") ~ "Africa",
    TRUE ~ "Diversified Multi-Regional"
  ))
final_modeling



```
getting time specific shocks for LPs

```{r}

lp_fund_counts <- final_modeling %>%
  group_by(lp) %>%
  summarise(num_funds = n_distinct(fund_id))


# View the first few rows of the summary
lp_fund_counts %>% arrange(desc(num_funds))

# Plot the distribution using a histogram
ggplot(lp_fund_counts, aes(x = num_funds)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Distribution of Total Number of Funds per LP",
    x = "Total Number of Funds",
    y = "Count of LPs"
  )
```



creating divergence metric

```{r}
# Set the analysis period from 2000 to 2022.
min_yr <- 2000
max_yr <- 2022

# Instead of filtering for funds from 2000 onward, include all funds with vintage <= max_yr.
final_modeling_filtered <- final_modeling %>%
  filter(!is.na(vintage_inception_year),
         vintage_inception_year <= max_yr)

expand_fund_periods_2yr <- function(lp, vintage, strat, min_yr, max_yr) {
  # Force vintage to be at least min_yr.
  start_yr <- max(vintage, min_yr)
  # Align the period: floor to the beginning of the 2‐year bin.
  period_start <- 1 * floor((start_yr - min_yr) / 1) + min_yr
  # Generate a sequence from period_start to max_yr in 2-year increments.
  yrs <- seq(period_start, max_yr, by = 1)
  tibble(
    lp = lp,
    period_2yr = yrs,   # Two-year bin identifier.
    strategy = strat,
    fund_count = 1      # Each row represents one fund.
  )
}

# Apply rowwise expansion.
expanded <- final_modeling_filtered %>%
  select(lp, vintage_inception_year, strategy) %>%
  rowwise() %>%
  do(
    expand_fund_periods_2yr(
      lp = .$lp,
      vintage = .$vintage_inception_year,
      strat = .$strategy,
      min_yr = min_yr,
      max_yr = max_yr
    )
  ) %>%
  ungroup()

# Aggregate funds by LP, period, and strategy (non-cumulative count).
lp_period_strategy <- expanded %>%
  group_by(lp, period_2yr, strategy) %>%
  summarize(num_funds = sum(fund_count, na.rm = TRUE), .groups = "drop")

# Calculate cumulative counts over time for each LP and each strategy.
lp_period_strategy_cum <- lp_period_strategy %>%
  arrange(lp, period_2yr) %>%
  group_by(lp, strategy) %>%
  mutate(cum_strategy = num_funds) %>% # here, num funds is already equal to cum_strategy!
  ungroup()

# Compute cumulative total funds (across all strategies) for each LP by period.
lp_period_totals <- lp_period_strategy %>%
  group_by(lp, period_2yr) %>%
  summarize(period_total = sum(num_funds, na.rm = TRUE), .groups = "drop") %>%
  arrange(lp, period_2yr) %>%
  group_by(lp) %>%
  mutate(cum_total_funds = period_total) %>%
  ungroup()


lp_period_strategy %>%
  group_by(lp, period_2yr) %>%
  summarize(period_total = sum(num_funds, na.rm = TRUE), .groups = "drop") %>%
  arrange(lp, period_2yr) %>%
  group_by(lp)
# Join the cumulative strategy counts with the cumulative totals and compute the strategy share.
lp_period_shares_cum <- lp_period_strategy_cum %>%
  left_join(lp_period_totals, by = c("lp", "period_2yr")) %>%
  mutate(
    strategy_share = if_else(num_funds > 0, cum_strategy / cum_total_funds, 0)
  )

# --- Divergence Measure: Proportions Divergence ---
# Pivot the cumulative proportion data to wide format.
lp_shares_prop <- lp_period_shares_cum %>%
  select(lp, period_2yr, strategy, strategy_share, cum_total_funds) %>%
  pivot_wider(names_from = strategy, values_from = strategy_share, values_fill = list(strategy_share = 0)) %>%
  arrange(lp, period_2yr)

# Join distinct cumulative total funds.
lp_totals <- lp_period_shares_cum %>%
  select(lp, period_2yr, cum_total_funds) %>%
  distinct()
lp_shares_prop <- lp_shares_prop %>%
  left_join(lp_totals, by = c("lp", "period_2yr"))

# Specify the strategy share columns.
prop_strategy_cols <- c("Buyout", "Early Stage Venture", "Venture (General)", "Secondaries", "Growth Equity", "Co-Investment", "Fund of Funds","Balanced", "Other")

# For each LP, compute the positive change in cumulative proportions between consecutive periods.
# (For each strategy, pmax(current - lag, 0)) and sum over strategies.
lp_shares_prop <- lp_shares_prop %>%
  arrange(lp) %>%
  arrange(period_2yr) %>%
  group_by(lp) %>%
  mutate(
    divergence_prop = rowSums(across(all_of(prop_strategy_cols), ~ pmax(. - dplyr::lag(.), 0)), na.rm = TRUE),
    normalized_div_prop = divergence_prop * sqrt(cum_total_funds.x)
  ) %>%
  ungroup()

colnames(lp_shares_prop)
# --- Summarize Proportions Divergence by Period ---
div_prop_summary <- lp_shares_prop %>%
  filter(!is.na(normalized_div_prop)) %>%
  group_by(period_2yr) %>%
  summarize(
    mean_norm_div_prop = mean(normalized_div_prop, na.rm = TRUE),
    sd_norm_div_prop = sd(normalized_div_prop, na.rm = TRUE),
    n = n(),
    se_norm_div_prop = sd_norm_div_prop / sqrt(n),
    lower_prop = mean_norm_div_prop - 1.96 * se_norm_div_prop,
    upper_prop = mean_norm_div_prop + 1.96 * se_norm_div_prop,
    .groups = "drop"
  )

# --- Plot the Proportions Divergence Measure ---
ggplot(div_prop_summary, aes(x = period_2yr, y = mean_norm_div_prop)) +
  geom_line(color = "darkgreen", size = 1) +
  geom_ribbon(aes(ymin = lower_prop, ymax = upper_prop), fill = "darkgreen", alpha = 0.2) +
  labs(
    title = "Mean Normalized Proportions Divergence Over Vintage Years",
    x = "Year",
    y = "Mean Normalized Proportions Divergence"
  ) +
  theme_minimal()

# write.csv(lp_shares_prop, "lp_shares_prop.csv")

```

visualizing strategies divergence over time
```{r}


# Assume lp_shares_prop is your wide-format dataframe with columns: 
# lp, period_2yr, and strategy columns (as defined in prop_strategy_cols).
# prop_strategy_cols should be defined as:
# prop_strategy_cols <- c( "Growth.Equity", "Secondaries", "Fund.of.Funds",
#                         "Co.Investment", "Early.Stage.Venture", "Balanced", "Other", "Venture..General.")
prop_strategy_cols <- c("Buyout", "Early Stage Venture", "Venture (General)", "Secondaries", "Growth Equity", "Co-Investment", "Fund of Funds","Balanced", "Other")

# Pivot the data to long format so that each row represents one LP, period, and strategy.
lp_shares_long <- lp_shares_prop %>%
  pivot_longer(
    cols = all_of(prop_strategy_cols),
    names_to = "strategy",
    values_to = "strategy_share"
  ) %>%
  group_by(lp, strategy) %>%
  arrange(period_2yr, .by_group = TRUE) %>%
  # Compute divergence for each strategy as the positive change from the previous period.
  mutate(
    divergence_indiv = pmax(strategy_share - dplyr::lag(strategy_share), 0)
  ) %>%
  ungroup()

# Now summarize the divergence for each strategy at each period (e.g., using median or mean)
div_strategy_summary <- lp_shares_long %>%
  filter(!is.na(divergence_indiv)) %>%  # drop the first period per LP-strategy (NA from lag)
  group_by(period_2yr, strategy) %>%
  summarize(
    mean_div = mean(divergence_indiv, na.rm = TRUE),
    sd_div = sd(divergence_indiv, na.rm = TRUE),
    n = n(),
    se_div = sd_div / sqrt(n),
    lower = mean_div - 1.96 * se_div,
    upper = mean_div + 1.96 * se_div,
    .groups = "drop"
  )

# Finally, plot the divergence over time with each line representing a strategy.
ggplot(div_strategy_summary, aes(x = period_2yr, y = mean_div, color = strategy)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = strategy), alpha = 0.2, color = NA) +
  labs(
    title = "Mean Divergence by Strategy Over Two-Year Periods",
    x = "Two-Year Period",
    y = "Mean Divergence (Positive Change in Proportion)"
  ) +
  theme_minimal()
```
exogneous variables and such
```{r}
lp_lptype <- final_modeling %>%
  select(lp, LPType) %>%
  distinct()

# Join the LPType information to lp_shares_prop.
lp_shares_prop_with_type <- lp_shares_prop %>%
  left_join(lp_lptype, by = "lp")

# (If not already computed, ensure that normalized_div_prop is available.
# You may have computed it as follows:)
lp_shares_prop_with_type <- lp_shares_prop_with_type %>%
  arrange(lp, period_2yr) %>%
  group_by(lp) %>%
  mutate(
    divergence_prop = rowSums(across(all_of(prop_strategy_cols), 
                                      ~ pmax(. - dplyr::lag(.), 0)), na.rm = TRUE),
    normalized_div_prop = divergence_prop * sqrt(cum_total_funds.x)
  ) %>%
  ungroup()

# Summarize the median normalized divergence (and, optionally, a 25th–75th percentile band)
div_lptype_summary <- lp_shares_prop_with_type %>%
  filter(!is.na(normalized_div_prop)) %>%  # drop first-period NAs from lag
  group_by(period_2yr, LPType) %>%
  summarize(
    median_div = mean(normalized_div_prop, na.rm = TRUE),
    lower = quantile(normalized_div_prop, 0.25, na.rm = TRUE),
    upper = quantile(normalized_div_prop, 0.75, na.rm = TRUE),
    .groups = "drop"
  )

# Plot the median divergence over time by LPType.
ggplot(div_lptype_summary, aes(x = period_2yr, y = median_div, color = LPType)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = LPType), 
              alpha = 0.2, color = NA) +
  labs(
    title = "Median Normalized Divergence Over Time by LPType",
    x = "Year",
    y = "Median Normalized Divergence"
  ) +
  theme_minimal()
```

```{r}
fed <- read.csv("/Users/mross/Desktop/fed_cleaned.csv")
spreads <- read.csv("/Users/mross/Desktop/Corporate Spreads.csv")
if (!require("quantmod")) {
  install.packages("quantmod")
  library(quantmod)
}
sp500_data <- getSymbols("^GSPC", src = "yahoo", from = "1990-01-01", auto.assign = FALSE)
sp500_returns <- dailyReturn(Cl(sp500_data))
pe_inflow <- read_xlsx("/Users/mross/Desktop/preqin_export_historicalFundraisingChart_2025_3_20.xlsx")

#### 1. Process Fed Rates Data ####
# Assume fed is a data frame with columns: Date (chr) and Rate (dbl)
fed_yearly <- fed %>%
  mutate(Date = as.Date(Date),
         year = year(Date)) %>%
  filter(year >= 2000, year <= 2022) %>%
  group_by(year) %>%
  summarize(fed_rate = mean(Rate, na.rm = TRUE), .groups = "drop")

#### 2. Process S&P 500 Returns Data ####
# Here we assume sp500_returns is an xts object that, when converted to a data frame,
# has a single column 'daily.returns' where each row is a string like "1990-01-02 0.0000000000".
# (If your data are already a data frame with one column, adjust accordingly.)
# First, convert the xts object to a data frame:
sp500_df <- data.frame(date = index(sp500_returns),
                       daily.returns = as.character(coredata(sp500_returns)[, 1]),
                       stringsAsFactors = FALSE)

# If the daily.returns column still contains both the date and the return (separated by space),
# separate them into two columns.
sp500_df <- sp500_df %>%
  mutate(year = as.numeric(year(date)), 
         daily.returns = as.numeric(daily.returns))

# Filter to the analysis period
sp500_df <- sp500_df %>% filter(year >= 1995, year <= 2022)

# Now, aggregate daily returns to annual (calendar-year) returns.
# For each year, we compound daily returns.
sp500_yearly <- sp500_df %>%
  group_by(year) %>%
  summarize(annual_return = prod(1 + daily.returns, na.rm = TRUE) - 1, .groups = "drop")

# Next, compute trailing returns. We define functions for 1-, 3-, and 5-year trailing returns.
# Here we use the annual returns. (You may choose to use actual trading-day returns instead.)
get_trailing_return <- function(year, annual_returns, window) {
  # get the subset of years: from (year - window + 1) to year
  yrs <- seq(year - window + 1, year)
  ret <- annual_returns %>% filter(year %in% yrs) %>% pull(annual_return)
  if(length(ret) < window) return(NA_real_)
  return(prod(1 + ret, na.rm = TRUE) - 1)
}

# Create trailing return columns (for each year end, trailing 1, 3, and 5 years)
sp500_yearly <- sp500_yearly %>%
  arrange(year) %>%
  mutate(trail_1yr = annual_return,
         trail_3yr = purrr::map_dbl(year, ~ get_trailing_return(.x, sp500_yearly, 3)),
         trail_5yr = purrr::map_dbl(year, ~ get_trailing_return(.x, sp500_yearly, 5))
  )

#### 3. Process PE Inflow Data ####
# Assume pe_inflow is already a data frame with columns: YEAR, NO. OF FUNDS, AGGREGATE CAPITAL RAISED (USD BN),
# AVERAGE SIZE (USD MN). We first ensure that the year column is numeric.
pe_inflow <- pe_inflow %>%
  rename(year = YEAR,
         num_funds = `NO. OF FUNDS`,
         capital_raised = `AGGREGATE CAPITAL RAISED (USD BN)`,
         avg_size = `AVERAGE SIZE (USD MN)`) %>%
  mutate(year = as.numeric(year)) %>%
  filter(year >= 2000, year <= 2022)


spreads_yearly <- spreads %>%
  mutate(observation_date = as.Date(observation_date),
         year = year(observation_date)) %>%
  filter(year >= 2000, year <= 2022) %>%
  group_by(year) %>%
  summarize(avg_spread = mean(BAMLC0A0CM, na.rm = TRUE), .groups = "drop")


#### 4. Combine All Exogenous Variables ####
# Now merge fed_yearly, sp500_yearly, and pe_inflow by year.
exogenous <- fed_yearly %>%
  full_join(sp500_yearly, by = "year") %>%
  full_join(pe_inflow, by = "year") %>% 
  full_join(spreads_yearly, by = "year") %>%
  arrange(year) %>% filter(year >= 2000)

# Print the exogenous dataset
print(exogenous)

#### (Optional) Plot the S&P 500 trailing returns over time ####
exogenous_long <- exogenous %>%
  select(year, trail_1yr, trail_3yr, trail_5yr) %>%
  pivot_longer(cols = starts_with("trail"), names_to = "trailing", values_to = "return")

ggplot(exogenous_long, aes(x = year, y = return, color = trailing)) +
  geom_line(size = 1) +
  labs(title = "S&P 500 Trailing Returns", x = "Year", y = "Trailing Return") +
  theme_minimal()

colnames(final_modeling)
```
more plots
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)

# Create a new dataset for LP-level manager performance using net_irr as the performance metric.
lp_perf_new <- final_modeling %>%
  group_by(lp) %>%
  summarize(manager_return = mean(net_irr, na.rm = TRUE), .groups = "drop")

# Calculate the 25th and 75th percentiles of manager performance.
q25 <- quantile(lp_perf_new$manager_return, 0.25, na.rm = TRUE)
q75 <- quantile(lp_perf_new$manager_return, 0.75, na.rm = TRUE)

# Create a new grouping variable based on these thresholds.
lp_perf_new <- lp_perf_new %>%
  mutate(manager_quartile = case_when(
    manager_return <= q25 ~ "Lower Quartile",
    manager_return >= q75 ~ "Upper Quartile",
    TRUE ~ "Middle"
  ))

# Do not modify final_modeling in place; instead, create a new dataset by joining the performance data
# to your divergence data (assumed to be in lp_shares_prop_with_type).
lp_shares_prop_with_type_new <- lp_shares_prop_with_type %>%
  left_join(lp_perf_new, by = "lp")

# Filter to keep only LPs whose managers are in the Upper or Lower quartile.
lp_shares_prop_ul <- lp_shares_prop_with_type_new %>%
  filter(manager_quartile %in% c("Upper Quartile", "Lower Quartile"))

# Summarize the median normalized divergence by period, LPType, and manager performance quartile.
div_summary_ul <- lp_shares_prop_ul %>%
  group_by(period_2yr, LPType, manager_quartile) %>%
  summarize(
    median_norm_div = median(normalized_div_prop, na.rm = TRUE),
    lower = quantile(normalized_div_prop, 0.25, na.rm = TRUE),
    upper = quantile(normalized_div_prop, 0.75, na.rm = TRUE),
    .groups = "drop"
  )

# Plot the median normalized divergence over time, faceted by LPType.
ggplot(div_summary_ul, aes(x = period_2yr, y = median_norm_div, color = manager_quartile)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = manager_quartile), alpha = 0.2, color = NA) +
  facet_wrap(~ LPType) +
  labs(
    title = "Median Normalized Divergence Over Time by Manager Performance Quartile",
    x = "Year",
    y = "Median Normalized Divergence",
    color = "Manager Performance",
    fill = "Manager Performance"
  ) +
  theme_minimal()

```

performance variables
```{r}
div_panel_model <- lp_shares_prop %>% select(lp, period_2yr, divergence_prop, normalized_div_prop, cum_total_funds.x)
div_panel_model <- left_join(div_panel_model, exogenous, by = c("period_2yr" = "year"))

fund_perf <- final_modeling_filtered %>%
  select(lp, vintage_inception_year, net_irr, 
         `median_benchmark_net_irr_%`, `average_benchmark_net_irr_%`) %>%
  mutate(
    net_irr = as.numeric(net_irr),
    `median_benchmark_net_irr_%` = as.numeric(`median_benchmark_net_irr_%`),
    `average_benchmark_net_irr_%` = as.numeric(`average_benchmark_net_irr_%`)
  )
# --------------------------------------------------
# Function to compute lagged performance (both median and mean).
compute_lagged_perf <- function(current_year, lp_id, gap, window_length = NA) {
  lower_bound <- if (!is.na(window_length)) current_year - window_length else -Inf
  upper_bound <- current_year - gap
  vals <- fund_perf %>%
    filter(lp == lp_id,
           vintage_inception_year >= lower_bound,
           vintage_inception_year <= upper_bound) %>%
    pull(net_irr)
  if (length(vals) == 0) {
    return(c(median = NA_real_, mean = NA_real_))
  } else {
    return(c(median = median(vals, na.rm = TRUE),
             mean = mean(vals, na.rm = TRUE)))
  }
}


# --------------------------------------------------
# New function to compute excess performance.
# For each fund in the window:
# - For the median aggregation, subtract the fund's median benchmark net IRR.
# - For the mean aggregation, subtract the fund's average benchmark net IRR.
compute_excess_perf_correct <- function(current_year, lp_id, gap, window_length = NA) {
  lower_bound <- if (!is.na(window_length)) current_year - window_length else -Inf
  upper_bound <- current_year - gap
  vals <- fund_perf %>%
    filter(lp == lp_id,
           vintage_inception_year >= lower_bound,
           vintage_inception_year <= upper_bound)
  if(nrow(vals) == 0) {
    return(c(median = NA_real_, mean = NA_real_))
  } else {
    # For median aggregation: use median_benchmark_net_irr_%
    excess_median <- vals$net_irr - vals$`median_benchmark_net_irr_%`
    # For mean aggregation: use average_benchmark_net_irr_%
    excess_mean   <- vals$net_irr - vals$`average_benchmark_net_irr_%`
    return(c(median = median(excess_median, na.rm = TRUE),
             mean = mean(excess_mean, na.rm = TRUE)))
  }
}

compute_lagged_mean_value <- function(current_year, lp_id, gap, window_length = NA, varname) {
  lower_bound <- if (!is.na(window_length)) current_year - window_length else -Inf
  upper_bound <- current_year - gap
  vals <- final_modeling_filtered %>%
    filter(lp == lp_id,
           vintage_inception_year >= lower_bound,
           vintage_inception_year <= upper_bound) %>%
    pull({{ varname }})
  if (length(vals) == 0) {
    return(NA_real_)
  } else {
    return(mean(vals, na.rm = TRUE))
  }
}


# --------------------------------------------------
# Calculate lagged performance and corrected excess performance columns.
div_panel_model <- div_panel_model %>%
  rowwise() %>%
  mutate(
    # Lagged performance: "win" and "nowin" versions with a 10-year window and unbounded window.
    lag2_win_med   = compute_lagged_perf(period_2yr, lp, gap = 2, window_length = 10)[["median"]],
    lag2_win_mean  = compute_lagged_perf(period_2yr, lp, gap = 2, window_length = 10)[["mean"]],
    lag2_nowin_med = compute_lagged_perf(period_2yr, lp, gap = 2, window_length = NA)[["median"]],
    lag2_nowin_mean= compute_lagged_perf(period_2yr, lp, gap = 2, window_length = NA)[["mean"]],
    lag3_win_med   = compute_lagged_perf(period_2yr, lp, gap = 3, window_length = 10)[["median"]],
    lag3_win_mean  = compute_lagged_perf(period_2yr, lp, gap = 3, window_length = 10)[["mean"]],
    lag3_nowin_med = compute_lagged_perf(period_2yr, lp, gap = 3, window_length = NA)[["median"]],
    lag3_nowin_mean= compute_lagged_perf(period_2yr, lp, gap = 3, window_length = NA)[["mean"]],
    lag4_win_med   = compute_lagged_perf(period_2yr, lp, gap = 4, window_length = 10)[["median"]],
    lag4_win_mean  = compute_lagged_perf(period_2yr, lp, gap = 4, window_length = 10)[["mean"]],
    lag4_nowin_med = compute_lagged_perf(period_2yr, lp, gap = 4, window_length = NA)[["median"]],
    lag4_nowin_mean= compute_lagged_perf(period_2yr, lp, gap = 4, window_length = NA)[["mean"]],
    # Corrected excess performance: use the new function to compute excess performance.
    lag2_excess_med  = compute_excess_perf_correct(period_2yr, lp, gap = 2, window_length = 10)[["median"]],
    lag2_excess_mean = compute_excess_perf_correct(period_2yr, lp, gap = 2, window_length = 10)[["mean"]],
    lag3_excess_med  = compute_excess_perf_correct(period_2yr, lp, gap = 3, window_length = 10)[["median"]],
    lag3_excess_mean = compute_excess_perf_correct(period_2yr, lp, gap = 3, window_length = 10)[["mean"]],
    lag4_excess_med  = compute_excess_perf_correct(period_2yr, lp, gap = 4, window_length = 10)[["median"]],
    lag4_excess_mean = compute_excess_perf_correct(period_2yr, lp, gap = 4, window_length = 10)[["mean"]],
    log_current_lag_avg_close_size = log(compute_lagged_mean_value(period_2yr, lp, gap = 1, window_length = NA, final_close_size_usd_mn)),
    log_current_lag_avg_manager_aum = log(compute_lagged_mean_value(period_2yr, lp, gap = 1, window_length = NA, fund_manager_total_aum_usd_mn)),
    current_lag_avg_close_size = compute_lagged_mean_value(period_2yr, lp, gap = 1, window_length = NA, final_close_size_usd_mn),
    current_lag_avg_manager_aum = compute_lagged_mean_value(period_2yr, lp, gap = 1, window_length = NA, fund_manager_total_aum_usd_mn),
    lag_fund_series = log(compute_lagged_mean_value(period_2yr, lp, gap = 1, window_length = NA, fund_number_series)),
  ) %>%
  ungroup()

# --------------------------------------------------
# Incorporate LP Characteristics and Preqin Ranking.

# Get LP-level characteristics.
lp_info <- final_modeling_filtered %>%
  select(lp, AUM_line, LPType) %>%
  distinct()
div_panel_model <- div_panel_model %>%
  left_join(lp_info, by = "lp")

# For each LP and period, calculate the cumulative average using only funds with vintage < period.

```
checking linearity of predictors for pr model
```{r}
# Load required libraries
library(dplyr)
library(plm)
library(lmtest)
library(sandwich)
library(ggplot2)
library(rlang)


# -------------------------------------------------------------------
# 1. Filter the panel dataset for periods from 2001 onward.
# -------------------------------------------------------------------
div_panel_model_filtered <- div_panel_model %>%
  filter(period_2yr >= 2001) %>%
  mutate(period_factor = factor(period_2yr))  # create a factor for period fixed effects


predictors <- c("fed_rate", "trail_3yr", "num_funds", "avg_size", "avg_spread", 
                "lag4_excess_med", "cum_total_funds.x")


# ----- 1. Plotting Macro-Level (Static) Predictors -----
# These variables are assumed to be the same for every LP in a given period.
static_predictors <- c("fed_rate", "trail_3yr", "num_funds", "avg_size", "avg_spread")

# Aggregate by period: for each period, take the first value of each predictor (since they are static)
# and calculate the mean normalized divergence (across LPs)
static_agg <- div_panel_model_filtered %>%
  group_by(period_2yr) %>%
  summarize(across(all_of(static_predictors), ~ dplyr::first(.), .names = "{.col}"),
            mean_div = mean(normalized_div_prop, na.rm = TRUE),
            .groups = "drop")

# Loop over each static predictor and create two plots: one for the raw predictor and one for the log-transformed predictor.
for (pred in static_predictors) {
  # Plot raw predictor vs. mean divergence
  p1 <- ggplot(static_agg, aes_string(x = pred, y = "mean_div")) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = paste("Mean Normalized Divergence vs", pred, "(Raw)"),
         x = pred, y = "Mean Normalized Divergence") +
    theme_minimal()
  print(p1)
  
  # Create a new column for the log-transformed predictor.
  static_agg <- static_agg %>%
    mutate(!!paste0("log_", pred) := log(.data[[pred]]))
  
  # Plot log-transformed predictor vs. mean divergence
  p2 <- ggplot(static_agg, aes_string(x = paste0("log_", pred), y = "mean_div")) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    labs(title = paste("Mean Normalized Divergence vs", pred, "(Log Transformed)"),
         x = paste("log(", pred, ")", sep = ""), y = "Mean Normalized Divergence") +
    theme_minimal()
  print(p2)
}

# ----- 2. Plotting Dynamic Predictors -----
# These variables vary by LP and period.
dynamic_predictors <- c("lag4_excess_med", "cum_total_funds.x")

for (pred in dynamic_predictors) {
  # Plot raw predictor vs. normalized divergence (scatter plot)
  p3 <- ggplot(div_panel_model_filtered, aes_string(x = pred, y = "normalized_div_prop")) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = paste("Normalized Divergence vs", pred, "(Raw)"),
         x = pred, y = "Normalized Divergence") +
    theme_minimal()
  print(p3)
  
  # Plot log-transformed predictor vs. normalized divergence.
  # Here we use aes_string() with an inline transformation.
  p4 <- ggplot(div_panel_model_filtered, aes_string(x = paste0("log(", pred, ")"), y = "normalized_div_prop")) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    labs(title = paste("Normalized Divergence vs", pred, "(Log Transformed)"),
         x = paste("log(", pred, ")", sep = ""), y = "Normalized Divergence") +
    theme_minimal()
  print(p4)
}

div_panel_model_filtered <- div_panel_model_filtered %>%
  mutate(log_cum_total_funds = log(cum_total_funds.x))

```

fixed effects model
```{r}

model_formula <- normalized_div_prop ~ fed_rate + trail_1yr + num_funds + avg_size + avg_spread + lag4_nowin_med + AUM_line + LPType + log_cum_total_funds + log_current_lag_avg_close_size + log_current_lag_avg_manager_aum + period_factor

# -------------------------------------------------------------------
# 3. Run the panel regression using plm (within estimator = fixed effects).
# -------------------------------------------------------------------
panel_model <- plm(model_formula, 
                   data = div_panel_model_filtered, 
                   index = c("lp", "period_2yr"),
                   model = "within", within = TRUE)

?plm()

summary(panel_model)

# -------------------------------------------------------------------
# 4. Obtain standard errors clustered at the LP level.
# -------------------------------------------------------------------


clustered_vcov_HC1 <- vcovHC(panel_model, type = "HC1", cluster = "group", method = "white1")
clustered_vcov_HC2 <- vcovHC(panel_model, type = "HC1", cluster = "group", method = "arellano")
clustered_vcov_HC3 <- vcovHC(panel_model, type = "HC3", cluster = "group")

?
# Compare coefficient tests:
cat("Using HC1:\n")
print(coeftest(panel_model, vcov = clustered_vcov_HC1))

cat("Using HC2:\n")
print(coeftest(panel_model, vcov = clustered_vcov_HC2))

cat("Using HC3:\n")
print(coeftest(panel_model, vcov = clustered_vcov_HC3))

# -------------------------------------------------------------------
# 5. Run Diagnostic Tests:
# -------------------------------------------------------------------
# (a) Breusch-Godfrey test for serial correlation in panel data.
bg_test <- pbgtest(panel_model)
print(bg_test)

# (b) Pesaran's test for cross-sectional dependence.
csd_test <- pcdtest(panel_model, test = "cd")
print(csd_test)

# (c) Breusch-Pagan test for heteroskedasticity.
bp_test <- bptest(panel_model)
print(bp_test)

summary(div_panel_model_filtered$normalized_div_prop)
?plm()
```
estimating size of fixed effects

```{r}
# Extract the fixed effects (LP-specific intercepts)
lp_fixed_effects <- fixef(panel_model)
# Convert them to a data frame
lp_fixed_effects_df <- tibble(
  lp = names(lp_fixed_effects),
  fixed_effect = as.numeric(lp_fixed_effects)
)

# Assume you have a dataset 'lp_info' with LP-level characteristics (e.g., LPType)
# If not, create it from your final_modeling_filtered:
lp_info <- final_modeling_filtered %>%
  select(lp, LPType) %>%
  distinct()

# Join the fixed effects with the LP characteristics
lp_fixed_effects_df <- lp_fixed_effects_df %>%
  left_join(lp_info, by = "lp")

# Now you can inspect or summarize the fixed effects by LPType.
# For example, compute the mean fixed effect by LPType:
lp_fixed_effects_summary <- lp_fixed_effects_df %>%
  group_by(LPType) %>%
  summarize(
    median_fixed_effect = median(fixed_effect, na.rm = TRUE),
    n = n()
  )
print(lp_fixed_effects_summary)

# Or plot the distribution of fixed effects by LPType:
ggplot(lp_fixed_effects_df, aes(x = LPType, y = fixed_effect)) +
  geom_boxplot() +
  labs(title = "Distribution of LP Fixed Effects by LP Type",
       x = "LP Type", y = "LP Fixed Effect") +
  theme_minimal()

```


random effects model

```{r}
library(plm)
library(lmtest)
library(sandwich)
library(dplyr)

# Filter the panel data to include only periods from 2001 onward.
div_panel_model_RE <- div_panel_model %>%
  filter(period_2yr >= 2001)

# Create a period fixed effect as a factor (if not already done)
div_panel_model_RE <- div_panel_model_RE %>%
  mutate(LPType = factor(LPType),
         period_factor = factor(period_2yr),
         log_fed_rate = log(fed_rate),
         log_trail_1yr = log(trail_1yr),
         log_capital_raised = log(capital_raised),
         log_avg_size = log(avg_size),
         log_avg_spread = log(avg_spread))
        

# Specify the regression formula.
# Note: 'LPType' should be a factor if it's categorical.
model_formula <- normalized_div_prop ~ log_fed_rate + trail_1yr + num_funds + 
  log_capital_raised + log_avg_size + log_avg_spread + lag4_win_med + AUM_line + LPType

# Estimate the random effects model using plm.
# The "random" model here allows the time-invariant variables to be estimated.
panel_model_re <- plm(model_formula, 
                      data = div_panel_model_RE, 
                      index = c("lp", "period_2yr"),
                      model = "random", random.method = "walhus")

# Display the summary of the random effects model.
summary(panel_model_re)

# Cluster standard errors at the LP level.
clustered_vcov_re <- vcovHC(panel_model_re, type = "HC1", cluster = "group")
coeftest(panel_model_re, vcov = clustered_vcov_re)
```

test for significance of any lagged performance indicator
```{r}
library(dplyr)
library(plm)
library(lmtest)
library(sandwich)

# Define your vector of performance metric names:
performance_metrics <- c("lag2_win_med", "lag2_win_mean",
                    "lag2_nowin_med", "lag2_nowin_mean",
                    "lag3_win_med", "lag3_win_mean",
                    "lag3_nowin_med", "lag3_nowin_mean",
                    "lag4_win_med", "lag4_win_mean",
                    "lag4_nowin_med", "lag4_nowin_mean",
                    "lag2_excess_med", "lag2_excess_mean",
                    "lag3_excess_med", "lag3_excess_mean",
                    "lag4_excess_med")


# Prepare an empty list to store results.
results_list <- list()

# Loop over each performance metric.
for(metric in performance_metrics){
  
  # Construct the model formula dynamically.
  # This replaces the performance metric in the baseline formula.
  current_formula <- as.formula(paste(
    "normalized_div_prop ~ fed_rate + trail_1yr + num_funds + ",
    "avg_size + avg_spread +", metric, "+ AUM_line + LPType + log_cum_total_funds + log_current_lag_avg_close_size + log_current_lag_avg_manager_aum + period_factor"
  ))
  
  # Run the panel regression using the 'within' estimator, 
  # now with fund-level fixed effects (assuming 'fund' is the fund identifier).
  current_model <- plm(current_formula, 
                       data = div_panel_model_filtered, 
                       index = c("lp", "period_2yr"),
                       model = "within")
  
  # Obtain robust standard errors clustered at the fund level.
  clustered_vcov_fund <- vcovHC(current_model, type = "HC1", cluster = "group")
  
  
  # Run the t-test on coefficients.
  test_result <- coeftest(current_model, vcov = clustered_vcov_fund)
  
  # Extract the row corresponding to the performance metric.
  if(metric %in% rownames(test_result)){
    metric_result <- test_result[metric, ]
    results_list[[metric]] <- metric_result
  } else {
    results_list[[metric]] <- NA  # In case the metric is not in the model (e.g., missing)
  }
}

# Combine all results into a single data frame.
results_df <- do.call(rbind, results_list)
print(results_df)

```

compare fixed effect and random effects
```{r}
library(plm)
library(lmtest)

# Suppose your panel data is in 'div_panel_model_filtered' 
# with individual index "lp" and time index "period_2yr"
# and your model formula is defined as:
model_formula <- normalized_div_prop ~ fed_rate + trail_1yr + num_funds +
  capital_raised + avg_size + avg_spread + lag4_excess_med + AUM_line + LPType

# Estimate the fixed effects model (within estimator)
fe_model <- plm(model_formula, 
                data = div_panel_model_filtered, 
                index = c("lp", "period_2yr"), 
                model = "within")

# Estimate the random effects model
re_model <- plm(model_formula, 
                data = div_panel_model_filtered, 
                index = c("lp", "period_2yr"), 
                model = "random",
                random.method = "walhus")

# Conduct the Hausman test:
hausman_result <- phtest(fe_model, re_model)
print(hausman_result)

```
### Second panel regression model -- geographic divergence
```{r}
# Load necessary packages
library(dplyr)
library(tidyr)

# Define your analysis period
min_yr <- 2000
max_yr <- 2022

# Define a helper function to expand each fund's vintage into yearly periods.
# Here, 'strat' is used to pass the geographic_focus value.
expand_fund_periods_2yr <- function(lp, vintage, strat, min_yr, max_yr) {
  # Ensure the fund's vintage is at least min_yr
  start_yr <- max(vintage, min_yr)
  # Create a sequence of years from start_yr to max_yr
  yrs <- seq(start_yr, max_yr, by = 1)
  tibble(
    lp = lp,
    period_2yr = yrs,   # This will serve as our time period indicator
    geo_focus = strat,
    fund_count = 1      # Each row represents one fund observation for that period
  )
}

# Assume your data is in a data frame called final_modeling_filtered with columns:
# lp, vintage_inception_year, and geographic_focus.

# Step 1: Expand funds into yearly periods for geographic focus.
expanded_geo <- final_modeling_filtered %>%
  select(lp, vintage_inception_year, geographic_focus) %>%
  rowwise() %>%
  do(expand_fund_periods_2yr(
    lp = .$lp,
    vintage = .$vintage_inception_year,
    strat = .$geographic_focus,
    min_yr = min_yr,
    max_yr = max_yr
  )) %>%
  ungroup()

# Step 2: Aggregate counts by LP, period, and geographic category.
lp_period_geo <- expanded_geo %>%
  group_by(lp, period_2yr, geo_focus) %>%
  summarise(num_funds = sum(fund_count, na.rm = TRUE), .groups = "drop")

# Step 3: Compute cumulative counts for each LP and geographic category.
lp_period_geo_cum <- lp_period_geo %>%
  arrange(lp, period_2yr) %>%
  group_by(lp, geo_focus) %>%
  mutate(cum_geo = num_funds) %>%  # Adjust this if you need a true cumulative sum
  ungroup()

# Step 4: Compute cumulative total funds by LP and period.
lp_period_totals <- lp_period_geo %>%
  group_by(lp, period_2yr) %>%
  summarise(period_total = sum(num_funds, na.rm = TRUE), .groups = "drop") %>%
  arrange(lp, period_2yr) %>%
  group_by(lp) %>%
  mutate(cum_total_funds = period_total) %>%  # Adjust if you need a cumulative sum over periods
  ungroup()

# Step 5: Merge cumulative geographic counts with total funds and compute geographic share.
lp_geo_shares <- lp_period_geo_cum %>%
  left_join(lp_period_totals, by = c("lp", "period_2yr")) %>%
  mutate(geo_share = if_else(cum_total_funds > 0, cum_geo / cum_total_funds, 0)) %>%
  select(lp, period_2yr, geo_focus, geo_share)

# Step 6: Pivot the data to wide format so that each geographic category is its own column,
# and join the cumulative total funds.
lp_geo_shares_wide <- lp_geo_shares %>%
  pivot_wider(
    names_from = geo_focus,
    values_from = geo_share
  ) %>%
  left_join(lp_period_totals %>% select(lp, period_2yr, cum_total_funds), 
            by = c("lp", "period_2yr"))

# Specify the geographic columns (update these names if needed)
geo_cols <- c("North America", "Asia", "Europe", "Australasia", 
              "Americas", "Middle East & Israel", "Africa", "Diversified Multi-Regional")

# Step 7: Compute divergence metric.
# For each LP, for each geographic column, calculate the positive change in share from the previous period,
# then sum these positive changes across all categories.
lp_geo_shares_wide <- lp_geo_shares_wide %>%
  arrange(lp, period_2yr) %>% 
  group_by(lp) %>%
  mutate(
    divergence_geo = rowSums(across(all_of(geo_cols), ~ pmax(. - dplyr::lag(.), 0)), na.rm = TRUE),
    normalized_div_geo = divergence_geo * sqrt(cum_total_funds)
  ) %>%
  ungroup()



```
```{r}
# # library(ggplot2)
# # 
# # Define the names of your response variables.
# response_vars <- c("Buyout", "Venture_General", "Secondaries",
#                    "Early Stage Venture", "Growth Equity", "Co-Investment",
#                    "new_manager_prop")
# 
# 
# 
# # Loop over each variable and create a histogram.
# for (var in response_vars) {
#   p <- ggplot(lp_emissions, aes(x = .data[[var]])) +
#     geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
#     labs(title = paste("Histogram of", var),
#          x = var,
#          y = "Count") +
#     theme_minimal()
#   print(p)
# }


```

```{r}
# library(dplyr)
# library(tidyr)
# library(purrr)
# library(depmixS4)
# library(lubridate)
# library(ggplot2)
# library(VGAM)
# 
# ### 1. Define Analysis Period and Bin Size ###
# min_yr <- 2000
# max_yr <- 2022
# step <- 2  # Using 2-year bins; adjust as desired
# 
# ### 2. Expand Fund-Level Data into Multi-Year Periods ###
# expand_fund_periods_4yr <- function(lp, vintage, strat, min_yr, max_yr, step) {
#   start_yr <- max(vintage, min_yr)
#   period_start <- min_yr + step * floor((start_yr - min_yr) / step)
#   yrs <- seq(period_start, max_yr, by = step)
#   tibble(
#     lp = lp,
#     period_4yr = yrs,
#     strategy = strat,
#     fund_count = 1
#   )
# }
# 
# final_modeling_filtered <- final_modeling %>%
#   filter(!is.na(vintage_inception_year),
#          vintage_inception_year <= max_yr)
# 
# expanded_4yr <- final_modeling_filtered %>%
#   select(lp, vintage_inception_year, strategy, fund_manager) %>%
#   rowwise() %>%
#   do(expand_fund_periods_4yr(
#     lp = .$lp,
#     vintage = .$vintage_inception_year,
#     strat = .$strategy,
#     min_yr = min_yr,
#     max_yr = max_yr,
#     step = step
#   )) %>%
#   ungroup()
# 
# ### 3. Aggregate Strategy Counts and Compute Proportions ###
# lp_period_strategy <- expanded_4yr %>%
#   group_by(lp, period_4yr, strategy) %>%
#   summarize(num_funds = sum(fund_count, na.rm = TRUE), .groups = "drop")
# 
# lp_period_totals <- lp_period_strategy %>%
#   group_by(lp, period_4yr) %>%
#   summarize(period_total = sum(num_funds, na.rm = TRUE), .groups = "drop")
# 
# lp_period_shares <- lp_period_strategy %>%
#   left_join(lp_period_totals, by = c("lp", "period_4yr")) %>%
#   mutate(strategy_share = if_else(period_total > 0, num_funds / period_total, 0)) %>%
#   select(lp, period_4yr, strategy, strategy_share)
# 
# selected_strategies <- c("Buyout", "Venture (General)", "Secondaries", 
#                          "Early Stage Venture", "Growth Equity", "Co-Investment")
# 
# lp_shares_wide <- lp_period_shares %>%
#   filter(strategy %in% selected_strategies) %>%
#   pivot_wider(
#     names_from = strategy,
#     values_from = strategy_share,
#     values_fill = list(strategy_share = 0)
#   )
# 
# ### 4. Calculate Cumulative New Manager Proportion ###
# lp_managers_period <- final_modeling_filtered %>%
#   select(lp, vintage_inception_year, fund_manager) %>%
#   mutate(
#     fund_manager = as.character(fund_manager),
#     period_4yr = min_yr + step * floor((vintage_inception_year - min_yr) / step)
#   ) %>%
#   distinct()
# 
# lp_managers_cum <- lp_managers_period %>%
#   group_by(lp, period_4yr) %>%
#   summarize(current_managers = list(unique(fund_manager)), .groups = "drop") %>%
#   arrange(lp, period_4yr) %>%
#   group_by(lp) %>%
#   mutate(
#     cum_managers_all = accumulate(current_managers, base::union),
#     cum_managers_prior = dplyr::lag(cum_managers_all, order_by = period_4yr, default = list(character(0)))
#   ) %>%
#   ungroup() %>%
#   mutate(
#     new_manager_count = map2_int(current_managers, cum_managers_prior, ~ length(setdiff(.x, .y))),
#     total_current = map_int(current_managers, length),
#     new_manager_prop = if_else(total_current > 0, new_manager_count / total_current, NA_real_)
#   ) %>%
#   select(lp, period_4yr, new_manager_prop)
# 
# ### 5. Merge Emissions Data ###
# lp_emissions <- lp_shares_wide %>%
#   left_join(lp_managers_cum, by = c("lp", "period_4yr")) %>%
#   left_join(lp_period_totals, by = c("lp", "period_4yr"))
# 
# lp_emissions <- lp_emissions %>%
#   rename(Venture_General = `Venture (General)`)
# 
# ### 6. Incorporate Exogenous Macro Variables + LP Characteristics ###
# lp_emissions <- lp_emissions %>%
#   mutate(year_bin = period_4yr) %>%
#   left_join(exogenous, by = c("year_bin" = "year"))
# 
# lp_chars <- final_modeling_filtered %>%
#   select(lp, AUM_line, LPType) %>%
#   distinct()
# 
# lp_emissions <- lp_emissions %>%
#   left_join(lp_chars, by = "lp") %>%
#   mutate(
#     LPType = factor(LPType),
#     AUM_line = as.numeric(AUM_line)
#   )
# 
# ### 7. Categorize new_manager_prop ###
# lp_emissions <- lp_emissions %>%
#   mutate(
#     new_manager_cat = case_when(
#       new_manager_prop < 0.25 ~ "low",
#       new_manager_prop < 0.75 ~ "medium",
#       TRUE ~ "high"
#     ),
#     new_manager_cat = factor(new_manager_cat, levels = c("low", "medium", "high"))
#   )
# 
# ### 8. Scale Covariates for Transition Modeling ###
# lp_emissions <- lp_emissions %>%
#   mutate(
#     fed_rate_scaled   = as.numeric(scale(fed_rate)),
#     trail_1yr_scaled  = as.numeric(scale(annual_return)),
#     avg_spread_scaled = as.numeric(scale(avg_spread)),
#     AUM_line_scaled   = as.numeric(scale(AUM_line))
#   )
# 
# ###############################################################################
# # CLIP ONLY VALUES >= 1, PRESERVE REAL ZEROES
# ###############################################################################
# clip_top <- function(x, eps = 1e-8) {
#   x_clipped <- ifelse(x >= 1, 1 - eps, x)
#   pmax(x_clipped, 0)
# }
# 
# lp_emissions <- lp_emissions %>%
#   mutate(
#     Buyout = clip_top(Buyout),
#     Venture_General = clip_top(Venture_General),
#     Secondaries = clip_top(Secondaries),
#     `Early Stage Venture` = clip_top(`Early Stage Venture`),
#     `Growth Equity` = clip_top(`Growth Equity`),
#     `Co-Investment` = clip_top(`Co-Investment`)
#   )
# 
# ###############################################################################
# # 8a. Create Lagged Variables for Each Emission
# ###############################################################################
# lp_emissions <- lp_emissions %>%
#   group_by(lp) %>%
#   arrange(period_4yr) %>%
#   mutate(
#     lag_Buyout = dplyr::lag(Buyout),
#     lag_Venture_General = dplyr::lag(Venture_General),
#     lag_Secondaries = dplyr::lag(Secondaries),
#     lag_Early_Stage_Venture = dplyr::lag(`Early Stage Venture`),
#     lag_Growth_Equity = dplyr::lag(`Growth Equity`),
#     lag_CoInvest = dplyr::lag(`Co-Investment`)
#   ) %>%
#   ungroup() %>%
#   filter(
#     !is.na(lag_Buyout),
#     !is.na(lag_Venture_General),
#     !is.na(lag_Secondaries),
#     !is.na(lag_Early_Stage_Venture),
#     !is.na(lag_Growth_Equity),
#     !is.na(lag_CoInvest)
#   )
# 
# ###############################################################################
# # 9. ZIBetaARResponse Class with AR(1) on pi, alpha, and beta,
# #    clamping pi to [eps, 1-eps]. pstart is set using moment estimates.
# ###############################################################################
# setClass("ZIBetaARResponse", contains = "response")
# 
# setGeneric("ZIBetaARResponse", function(y, pstart = NULL, fixed = NULL, x = NULL, ...) 
#   standardGeneric("ZIBetaARResponse"))
# 
# setMethod("ZIBetaARResponse", signature(y = "ANY"),
#   function(y, pstart = NULL, fixed = NULL, x = NULL, ...) {
#     y <- matrix(y, ncol = 1)
#     if (is.null(x)) {
#       x <- cbind(rep(1, nrow(y)), rep(0, nrow(y)))
#     } else {
#       x <- as.matrix(x)
#     }
#     # 6 parameters: p0, p1 for logit(pi); a0, a1 for log(alpha); b0, b1 for log(beta)
#     npar <- 6
#     if (is.null(pstart)) {
#       pstart <- c(-1, 0, log(2), 0, log(5), 0)
#     }
#     if (is.null(fixed)) fixed <- rep(FALSE, npar)
#     parameters <- list(
#       p0 = pstart[1], p1 = pstart[2],
#       a0 = pstart[3], a1 = pstart[4],
#       b0 = pstart[5], b1 = pstart[6]
#     )
#     new("ZIBetaARResponse", parameters = parameters, fixed = fixed, x = x, y = y, npar = npar)
#   }
# )
# 
# setMethod("show", "ZIBetaARResponse", function(object) {
#   cat("ZIBetaARResponse:\n")
#   cat("p0:", object@parameters$p0, " p1:", object@parameters$p1, "\n")
#   cat("a0:", object@parameters$a0, " a1:", object@parameters$a1, "\n")
#   cat("b0:", object@parameters$b0, " b1:", object@parameters$b1, "\n")
# })
# 
# setMethod("dens", "ZIBetaARResponse", function(object, log = FALSE) {
#   y_val <- object@y[,1]
#   X <- object@x  # Nx2: col2 = lag
#   p0 <- object@parameters$p0
#   p1 <- object@parameters$p1
#   a0 <- object@parameters$a0
#   a1 <- object@parameters$a1
#   b0 <- object@parameters$b0
#   b1 <- object@parameters$b1
#   
#   # Compute pi with logistic and clamp it.
#   logit_vals <- p0 + p1 * X[,2]
#   pi_raw <- 1 / (1 + exp(-logit_vals))
#   eps <- 1e-8
#   pi_vals <- pmin(pmax(pi_raw, eps), 1 - eps)
#   
#   alpha_vals <- exp(a0 + a1 * X[,2])
#   beta_vals  <- exp(b0 + b1 * X[,2])
#   
#   if (!log) {
#     out <- ifelse(y_val == 0,
#                   pi_vals,
#                   (1 - pi_vals) * dbeta(y_val, shape1 = alpha_vals, shape2 = beta_vals))
#     out
#   } else {
#     out <- ifelse(y_val == 0,
#                   log(pi_vals),
#                   log(1 - pi_vals) + dbeta(y_val, shape1 = alpha_vals, shape2 = beta_vals, log = TRUE))
#     out
#   }
# })
# 
# setMethod("getpars", "ZIBetaARResponse", function(object, which = "pars", ...) {
#   switch(which,
#          "pars" = c(object@parameters$p0, object@parameters$p1,
#                     object@parameters$a0, object@parameters$a1,
#                     object@parameters$b0, object@parameters$b1),
#          "fixed" = object@fixed)
# })
# 
# setMethod("setpars", "ZIBetaARResponse", function(object, values, which = "pars", ...) {
#   if (length(values) != object@npar) stop("length of 'values' must be ", object@npar)
#   if (which == "pars") {
#     object@parameters$p0 <- values[1]
#     object@parameters$p1 <- values[2]
#     object@parameters$a0 <- values[3]
#     object@parameters$a1 <- values[4]
#     object@parameters$b0 <- values[5]
#     object@parameters$b1 <- values[6]
#   } else if (which == "fixed") {
#     object@fixed <- as.logical(values)
#   }
#   object
# })
# 
# setMethod("fit", "ZIBetaARResponse", function(object, w) {
#   if (missing(w) || is.null(w)) w <- rep(1, nrow(object@y))
#   nll <- function(par) {
#     object_temp <- setpars(object, par, which = "pars")
#     dvals <- dens(object_temp, log = TRUE)
#     sum(-w * dvals)
#   }
#   init_par <- getpars(object, "pars")
#   fit_out <- optim(init_par, nll, method = "BFGS")
#   object <- setpars(object, fit_out$par, which = "pars")
#   object
# })
# 
# setMethod("predict", "ZIBetaARResponse", function(object) {
#   X <- object@x
#   eps <- 1e-8
#   pi_raw <- 1 / (1 + exp(-(object@parameters$p0 + object@parameters$p1 * X[,2])))
#   pi_vals <- pmin(pmax(pi_raw, eps), 1 - eps)
#   
#   alpha_vals <- exp(object@parameters$a0 + object@parameters$a1 * X[,2])
#   beta_vals  <- exp(object@parameters$b0 + object@parameters$b1 * X[,2])
#   
#   (1 - pi_vals) * (alpha_vals / (alpha_vals + beta_vals))
# })
# 
# ZIBetaAR <- function(y, pstart = c(-1, 0, log(2), 0, log(5), 0), fixed = NULL, x = NULL) {
#   ZIBetaARResponse(y = y, pstart = pstart, fixed = fixed, x = x)
# }
# 
# ###############################################################################
# # 10c. Build rModels Using ZIBetaAR (AR in pi, alpha, and beta) with Moment-based pstart
# ###############################################################################
# # This function uses compute_ZIBeta_moments_AR() to set pstart based on the moments.
# compute_ZIBeta_moments_AR <- function(x) {
#   p0 <- mean(x == 0, na.rm = TRUE)
#   p0 <- min(max(p0, 1e-8), 1 - 1e-8)
#   logit_p0 <- log(p0 / (1 - p0))
#   nonzero <- x[x > 0]
#   if(length(nonzero) < 2) {
#     alpha <- 2; beta <- 5
#   } else {
#     m <- mean(nonzero)
#     v <- var(nonzero)
#     if(v > 0) {
#       t_val <- (m * (1 - m) / v) - 1
#       alpha <- m * t_val
#       beta <- (1 - m) * t_val
#       if(alpha <= 0) alpha <- 2
#       if(beta <= 0) beta <- 5
#     } else {
#       alpha <- 2; beta <- 5
#     }
#   }
#   c(logit_p0, 0, log(alpha), 0, log(beta), 0)
# }
# 
# build_rModels <- function(M, data) {
#   rModels <- vector("list", M)
#   
#   # Debug helper: test log-likelihood of a response
#   test_pstart_ll <- function(respObj) {
#     dvals <- dens(respObj, log = TRUE)
#     if(any(is.infinite(dvals))) cat("Warning: infinite log-likelihood detected!\n")
#     if(any(is.na(dvals))) cat("Warning: NA in log-likelihood detected!\n")
#   }
#   
#   for (i in seq_len(M)) {
#     # For each emission, get moment-based pstart using the function above.
#     buy_pstart <- compute_ZIBeta_moments_AR(data$Buyout)
#     vent_pstart <- compute_ZIBeta_moments_AR(data$Venture_General)
#     sec_pstart <- compute_ZIBeta_moments_AR(data$Secondaries)
#     esv_pstart <- compute_ZIBeta_moments_AR(data$`Early Stage Venture`)
#     ge_pstart <- compute_ZIBeta_moments_AR(data$`Growth Equity`)
#     ci_pstart <- compute_ZIBeta_moments_AR(data$`Co-Investment`)
#     
#     cat("[DEBUG] Buyout pstart:\n"); print(buy_pstart)
#     cat("[DEBUG] Venture pstart:\n"); print(vent_pstart)
#     cat("[DEBUG] Secondaries pstart:\n"); print(sec_pstart)
#     cat("[DEBUG] Early Stage pstart:\n"); print(esv_pstart)
#     cat("[DEBUG] Growth Equity pstart:\n"); print(ge_pstart)
#     cat("[DEBUG] Co-Investment pstart:\n"); print(ci_pstart)
#     
#     # Build the responses with AR design matrix (intercept and lag)
#     buyResp <- ZIBetaAR(
#       y = data$Buyout,
#       pstart = buy_pstart,
#       x = cbind(1, data$lag_Buyout)
#     )
#     test_pstart_ll(buyResp)
#     
#     ventResp <- ZIBetaAR(
#       y = data$Venture_General,
#       pstart = vent_pstart,
#       x = cbind(1, data$lag_Venture_General)
#     )
#     test_pstart_ll(ventResp)
#     
#     secResp <- ZIBetaAR(
#       y = data$Secondaries,
#       pstart = sec_pstart,
#       x = cbind(1, data$lag_Secondaries)
#     )
#     test_pstart_ll(secResp)
#     
#     esvResp <- ZIBetaAR(
#       y = data$`Early Stage Venture`,
#       pstart = esv_pstart,
#       x = cbind(1, data$lag_Early_Stage_Venture)
#     )
#     test_pstart_ll(esvResp)
#     
#     geResp <- ZIBetaAR(
#       y = data$`Growth Equity`,
#       pstart = ge_pstart,
#       x = cbind(1, data$lag_Growth_Equity)
#     )
#     test_pstart_ll(geResp)
#     
#     ciResp <- ZIBetaAR(
#       y = data$`Co-Investment`,
#       pstart = ci_pstart,
#       x = cbind(1, data$lag_CoInvest)
#     )
#     test_pstart_ll(ciResp)
#     
#     multiResp <- GLMresponse(
#       formula = new_manager_cat ~ 1,
#       data = data,
#       family = depmixS4::multinomial("identity"),
#       pstart = c(0.33, 0.33, 0.33)
#     )
#     
#     rModels[[i]] <- list(buyResp, ventResp, secResp, esvResp, geResp, ciResp, multiResp)
#   }
#   rModels
# }
# 
# trans_formula <- ~ fed_rate_scaled + trail_1yr_scaled + avg_spread_scaled + AUM_line_scaled + LPType
# 
# ntimes_vec <- lp_emissions %>%
#   group_by(lp) %>%
#   summarize(T = n(), .groups = "drop") %>%
#   pull(T)
# 
# max_states <- 4
# model_list <- list()
# crit_table <- data.frame(nstates = integer(), logLik = numeric(), AIC = numeric(), BIC = numeric())
# 
# for (m in 2:max_states) {
#   cat("Fitting model with", m, "states...\n")
#   
#   rModels_m <- build_rModels(m, lp_emissions)
#   lp_prior_data <- lp_emissions %>% distinct(lp)
#   
#   trans_list <- lapply(seq_len(m), function(i) {
#     transInit(trans_formula, nstates = m, data = lp_emissions)
#   })
#   
#   prior_model <- transInit(
#     ~ 1,
#     ns = m,
#     data = lp_prior_data,
#     family = depmixS4::multinomial("identity")
#   )
#   
#   mod_spec <- makeDepmix(
#     response = rModels_m,
#     transition = trans_list,
#     prior = prior_model,
#     ntimes = ntimes_vec,
#     homogeneous = FALSE
#   )
#   
#   set.seed(492)
#   mod_fit <- fit(mod_spec, verbose = FALSE)
#   
#   llval <- logLik(mod_fit)
#   np <- npar(mod_fit)
#   n_tot <- sum(ntimes_vec)
#   
#   AIC_val <- -2 * llval + 2 * np
#   BIC_val <- -2 * llval + np * log(n_tot)
#   
#   model_list[[as.character(m)]] <- mod_fit
#   crit_table <- rbind(
#     crit_table,
#     data.frame(
#       nstates = m,
#       logLik = as.numeric(llval),
#       AIC = as.numeric(AIC_val),
#       BIC = as.numeric(BIC_val)
#     )
#   )
# }
# 
# cat("Model selection criteria:\n")
# print(crit_table)
# 
# best_nstates <- crit_table %>%
#   arrange(BIC) %>%
#   slice(1) %>%
#   pull(nstates)
# 
# cat("Best model has", best_nstates, "states.\n")
# 
# best_model <- model_list[[as.character(best_nstates)]]
# summary(best_model)
# 
# # Posterior decoding
# post_res <- posterior(best_model, type = "viterbi")
# table(post_res$state)
# 
# # Plot model selection criteria
# crit_long <- crit_table %>%
#   pivot_longer(cols = c(AIC, BIC), names_to = "criterion", values_to = "value")
# 
# ggplot(crit_long, aes(x = nstates, y = value, color = criterion)) +
#   geom_line(size = 1) +
#   geom_point(size = 2) +
#   labs(title = "Model Selection Criteria vs. Number of States",
#        x = "Number of States",
#        y = "Criterion Value",
#        color = "Criterion") +
#   scale_color_manual(values = c("AIC" = "blue", "BIC" = "red")) +
#   theme_minimal()
# 
# # -----------------------------------------------
# # Additional Diagnostics: Pseudo-Residuals for Venture_General
# # -----------------------------------------------
# post_df <- posterior(best_model)
# best_nstates <- ncol(post_df) - 1
# state_probs <- post_df %>% select(-c("state"))
# 
# F_state <- function(x, p0, alpha, beta) {
#   if (x == 0) {
#     return(p0)
#   } else {
#     return(p0 + (1 - p0) * pbeta(x, shape1 = alpha, shape2 = beta))
#   }
# }
# 
# params_list <- lapply(1:best_nstates, function(s) {
#   resp_obj <- best_model@response[[s]][[2]]
#   pars <- getpars(resp_obj, "pars")
#   list(p0 = plogis(pars[1]), alpha = exp(pars[2]), beta = exp(pars[3]))
# })
# 
# F_t <- numeric(nrow(lp_emissions))
# for (t in 1:nrow(lp_emissions)) {
#   x_t <- lp_emissions$Venture_General[t]
#   Fs_t <- sapply(1:best_nstates, function(s) {
#     par <- params_list[[s]]
#     F_state(x_t, p0 = par$p0, alpha = par$alpha, beta = par$beta)
#   })
#   gamma_t <- as.numeric(state_probs[t, ])
#   F_t[t] <- sum(gamma_t * Fs_t)
# }
# z_t <- qnorm(F_t)
# 
# ggplot(data.frame(z = z_t), aes(x = z)) +
#   geom_histogram(aes(y = ..density..), bins = 30, fill = "grey", color = "black") +
#   stat_function(fun = dnorm, color = "red", size = 1) +
#   labs(title = "Histogram of Normal Pseudo-Residuals",
#        x = "Pseudo-Residual (z)", y = "Density") +
#   theme_minimal()
# 
# ggplot(data.frame(z = z_t), aes(sample = z)) +
#   stat_qq() +
#   stat_qq_line(color = "red") +
#   labs(title = "QQ-Plot of Normal Pseudo-Residuals",
#        x = "Theoretical Quantiles", y = "Sample Quantiles") +
#   theme_minimal()
# 
# acf(z_t, main = "ACF of Normal Pseudo-Residuals for Venture_General")
# 
# lb_test <- Box.test(z_t, lag = 15, type = "Ljung-Box")
# print(lb_test)
# 
# # -----------------------------------------------
# # Append state assignment to lp_emissions and plot state-specific densities
# # -----------------------------------------------
# post_res <- posterior(best_model, type = "viterbi")
# lp_emissions <- lp_emissions %>% mutate(state = post_res$state)
# 
# params_list <- lapply(1:best_nstates, function(s) {
#   resp_obj <- best_model@response[[s]][[2]]
#   pars <- getpars(resp_obj, "pars")
#   list(p0 = plogis(pars[1]), alpha = exp(pars[2]), beta = exp(pars[3]))
# })
# 
# params_df <- do.call(rbind, lapply(1:length(params_list), function(s) {
#   data.frame(state = s,
#              p0 = params_list[[s]]$p0,
#              alpha = params_list[[s]]$alpha,
#              beta = params_list[[s]]$beta)
# }))
# print(params_df)
# 
# data_nonzero <- lp_emissions %>% filter(Venture_General > 0)
# 
# density_data <- do.call(rbind, lapply(unique(data_nonzero$state), function(s) {
#   state_data <- data_nonzero %>% filter(state == s)
#   par <- params_list[[s]]
#   x_seq <- seq(min(state_data$Venture_General), max(state_data$Venture_General), length.out = 100)
#   dens <- (1 - par$p0) * dbeta(x_seq, shape1 = par$alpha, shape2 = par$beta)
#   data.frame(state = s, x = x_seq, density = dens)
# }))
# 
# ggplot(data_nonzero, aes(x = Venture_General)) +
#   geom_histogram(aes(y = ..density..), bins = 30, fill = "grey", color = "black", alpha = 0.6) +
#   geom_line(data = density_data, aes(x = x, y = density), color = "red", size = 1) +
#   facet_wrap(~ state, scales = "free") +
#   labs(title = "State-specific Fitted Beta Density for Venture_General",
#        x = "Venture_General", y = "Density")
# 

```

```{r}
library(dplyr)
library(tidyr)
library(purrr)
library(depmixS4)
library(lubridate)
library(ggplot2)
library(VGAM)

### 1. Define Analysis Period and Bin Size ###
min_yr <- 2000
max_yr <- 2022
step   <- 2  # 2-year bins

### 2. Process Fund-Level Data: Assign Each Fund to Its Entry Period ###
# Each fund is assigned to the period when it was added.
funds <- final_modeling %>%
  filter(!is.na(vintage_inception_year), vintage_inception_year <= max_yr) %>%
  mutate(period_4yr = min_yr + step * floor((vintage_inception_year - min_yr) / step))

### 3. Aggregate New Fund Counts by LP, Period, and Strategy ###
lp_period_strategy <- funds %>%
  group_by(lp, period_4yr, strategy) %>%
  summarize(new_funds = n(), .groups = "drop")

lp_period_totals <- lp_period_strategy %>%
  group_by(lp, period_4yr) %>%
  summarize(period_total = sum(new_funds, na.rm = TRUE), .groups = "drop")

selected_strategies <- c("Buyout", "Venture (General)", "Secondaries",
                         "Early Stage Venture", "Growth Equity", "Co-Investment")

lp_counts_wide <- lp_period_strategy %>%
  filter(strategy %in% selected_strategies) %>%
  pivot_wider(
    names_from  = strategy,
    values_from = new_funds,
    values_fill = list(new_funds = 0)
  )

### 4. Calculate Cumulative New Manager Proportion ###
lp_managers_period <- final_modeling_filtered %>%
  select(lp, vintage_inception_year, fund_manager) %>%
  mutate(
    fund_manager = as.character(fund_manager),
    period_4yr   = min_yr + step * floor((vintage_inception_year - min_yr) / step)
  ) %>%
  distinct()

lp_managers_cum <- lp_managers_period %>%
  group_by(lp, period_4yr) %>%
  summarize(current_managers = list(unique(fund_manager)), .groups = "drop") %>%
  arrange(lp, period_4yr) %>%
  group_by(lp) %>%
  mutate(
    cum_managers_all   = accumulate(current_managers, base::union),
    cum_managers_prior = dplyr::lag(cum_managers_all, order_by = period_4yr, default = list(character(0)))
  ) %>%
  ungroup() %>%
  mutate(
    new_manager_count = map2_int(current_managers, cum_managers_prior, ~ length(setdiff(.x, .y))),
    total_current     = map_int(current_managers, length),
    new_manager_prop  = if_else(total_current > 0, new_manager_count / total_current, NA_real_)
  ) %>%
  select(lp, period_4yr, new_manager_prop)

### 5. Merge Emissions Data ###
lp_emissions <- lp_counts_wide %>%
  left_join(lp_managers_cum, by = c("lp", "period_4yr")) %>%
  left_join(lp_period_totals, by = c("lp", "period_4yr"))

# Rename "Venture (General)" to Venture_General for ease.
lp_emissions <- lp_emissions %>%
  rename(Venture_General = `Venture (General)`)

### 6. Incorporate Exogenous Macro Variables + LP Characteristics ###
lp_emissions <- lp_emissions %>%
  mutate(year_bin = period_4yr) %>%
  left_join(exogenous, by = c("year_bin" = "year"))

lp_chars <- final_modeling_filtered %>%
  select(lp, AUM_line, LPType) %>%
  distinct()

lp_emissions <- lp_emissions %>%
  left_join(lp_chars, by = "lp") %>%
  mutate(
    LPType   = factor(LPType),
    AUM_line = as.numeric(AUM_line)
  )

### 7. (Optional) Categorize New Manager Proportion (for reference)
lp_emissions <- lp_emissions %>%
  mutate(
    new_manager_cat = case_when(
      new_manager_prop < 0.25 ~ "low",
      new_manager_prop < 0.75 ~ "medium",
      TRUE ~ "high"
    ),
    new_manager_cat = factor(new_manager_cat, levels = c("low", "medium", "high"))
  )

### 8. Scale Covariates for Transition Modeling ###
lp_emissions <- lp_emissions %>%
  mutate(
    fed_rate_scaled   = as.numeric(scale(fed_rate)),
    trail_1yr_scaled  = as.numeric(scale(annual_return)),
    avg_spread_scaled = as.numeric(scale(avg_spread)),
    AUM_line_scaled   = as.numeric(scale(AUM_line))
  )

### 9. Create Lagged Variables (if needed) – Not required for new fund counts ###
# (Here we do not include lagged predictors because we focus on counts per period.)

###############################################################################
# 10. Force Count Variables to Integer (Poisson requires integer counts)
###############################################################################
lp_emissions <- lp_emissions %>%
  mutate(
    Buyout             = as.integer(round(Buyout)),
    Venture_General    = as.integer(round(Venture_General)),
    Secondaries        = as.integer(round(Secondaries)),
    `Early Stage Venture` = as.integer(round(`Early Stage Venture`)),
    `Growth Equity`       = as.integer(round(`Growth Equity`)),
    `Co-Investment`       = as.integer(round(`Co-Investment`))
  )

###############################################################################
# 11. Define a Flat List of Response Formulas and Matching Family List
###############################################################################
response_formulas <- list(
  Buyout ~ 1,
  Venture_General ~ 1,
  Secondaries ~ 1,
  `Early Stage Venture` ~ 1,
  `Growth Equity` ~ 1,
  `Co-Investment` ~ 1,
  new_manager_cat ~ 1
)

fam_list <- list(
  poisson(),                      # Buyout
  poisson(),                      # Venture_General
  poisson(),                      # Secondaries
  poisson(),                      # Early Stage Venture
  poisson(),                      # Growth Equity
  poisson(),                      # Co-Investment
  depmixS4::multinomial("identity")  # new_manager_cat
)

###############################################################################
# 12. Filter Out LPs with Only One Time Point (needed for transitions)
###############################################################################
lp_emissions <- lp_emissions %>%
  group_by(lp) %>%
  filter(n() > 1) %>%
  ungroup()

lp_emissions <- lp_emissions %>%
  filter(period_4yr >= min_yr & period_4yr <= max_yr)

###############################################################################
# 13. Compute ntimes (Number of Periods per LP)
###############################################################################
ntimes_vec <- lp_emissions %>%
  group_by(lp) %>%
  summarize(T = n(), .groups = "drop") %>%
  pull(T)

###############################################################################
# 14. Set Transition Formula (including new_manager_prop as a continuous predictor)
###############################################################################
trans_formula <- ~ fed_rate_scaled + trail_1yr_scaled + avg_spread_scaled + AUM_line_scaled + LPType

###############################################################################
# 15. Build HMM Using depmix() with a Flat List of Responses
###############################################################################
max_states <- 6
model_list <- list()
crit_table <- data.frame(nstates = integer(), logLik = numeric(),
                         AIC = numeric(), BIC = numeric())

for(m in 2:max_states) {
  cat("Fitting model with", m, "states...\n")
  
  mod_spec <- depmix(
    response    = response_formulas,
    data        = lp_emissions,
    nstates     = m,
    family      = fam_list,
    ntimes      = ntimes_vec,
    transition  = trans_formula,
    prior       = ~1,
    homogeneous = FALSE
  )
  
  set.seed(492)
  mod_fit <- fit(mod_spec, verbose = TRUE)
  
  llval <- logLik(mod_fit)
  np    <- npar(mod_fit)
  n_tot <- sum(ntimes_vec)
  
  AIC_val <- -2 * llval + 2 * np
  BIC_val <- -2 * llval + np * log(n_tot)
  
  model_list[[as.character(m)]] <- mod_fit
  crit_table <- rbind(
    crit_table,
    data.frame(
      nstates = m,
      logLik  = as.numeric(llval),
      AIC     = as.numeric(AIC_val),
      BIC     = as.numeric(BIC_val)
    )
  )
}

cat("Model selection criteria:\n")
print(crit_table)

best_nstates <- crit_table %>%
  arrange(BIC) %>%
  slice(1) %>%
  pull(nstates)

cat("Best model has", best_nstates, "states.\n")

best_model <- model_list[[as.character(best_nstates)]]
summary(best_model)

###############################################################################
# 16. Posterior Decoding and Diagnostics for the "Buyout" Poisson Response
###############################################################################
post_res <- posterior(best_model, type = "viterbi")
table(post_res$state)

crit_long <- crit_table %>%
  pivot_longer(cols = c(AIC, BIC), names_to = "criterion", values_to = "value")

ggplot(crit_long, aes(x = nstates, y = value, color = criterion)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "Poisson HMM: Model Selection Criteria vs. Number of States",
       x = "Number of States", y = "Criterion Value", color = "Criterion") +
  scale_color_manual(values = c("AIC" = "blue", "BIC" = "red")) +
  theme_minimal()

###############################################################################
# 17. Pseudo-Residuals for "Buyout" (the 1st Poisson Response)
###############################################################################
post_df <- posterior(best_model)
# The first column is the Viterbi state; remaining columns are state probabilities.
best_nstates <- ncol(post_df) - 1  
state_probs  <- post_df %>% select(-c("state"))

Buyout_val <- lp_emissions$Buyout

# Extract lambda for Buyout from the first response in each state.
params_list <- lapply(seq_len(best_nstates), function(s) {
  resp_obj <- best_model@response[[s]][[1]]
  par_int  <- getpars(resp_obj, which = "pars")
  exp(par_int[1])  # lambda_s for state s
})

F_mix <- numeric(nrow(lp_emissions))
for(t in seq_len(nrow(lp_emissions))) {
  x_t <- Buyout_val[t]
  Fs_t <- sapply(seq_len(best_nstates), function(s) {
    lam_s <- params_list[[s]]
    ppois(x_t, lam_s)
  })
  gamma_t <- as.numeric(state_probs[t, ])
  F_mix[t] <- sum(gamma_t * Fs_t)
}

z_t <- qnorm(F_mix)

ggplot(data.frame(z = z_t), aes(x = z)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "grey", color = "black") +
  stat_function(fun = dnorm, color = "red", size = 1) +
  labs(title = "Histogram of Normal Pseudo-Residuals (Buyout Poisson)",
       x = "Pseudo-Residual (z)", y = "Density") +
  theme_minimal()

ggplot(data.frame(z = z_t), aes(sample = z)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ-Plot of Normal Pseudo-Residuals (Buyout Poisson)",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

acf(z_t, main = "ACF of Normal Pseudo-Residuals (Buyout)")

lb_test <- Box.test(z_t, lag = 15, type = "Ljung-Box")
print(lb_test)

###############################################################################
# 18. Append State Assignment to lp_emissions and Display State Counts
###############################################################################
post_res <- posterior(best_model, type = "viterbi")
lp_emissions <- lp_emissions %>% mutate(state = post_res$state)
table(lp_emissions$state)


```
```{r}
library(dplyr)
library(ggplot2)

# Define the names of the response variables (as they appear in your data)
response_vars <- c("Buyout", "Venture_General", "Secondaries", 
                   "Early Stage Venture", "Growth Equity", "Co-Investment")

# Ensure they are stored as integers
lp_emissions <- lp_emissions %>%
  mutate(across(all_of(response_vars), ~ as.integer(round(.))))


# Loop over each response variable and print an individual plot
for (var in response_vars) {
  # Compute the mean (lambda) for the variable
  lambda <- mean(lp_emissions[[var]], na.rm = TRUE)
  
  # Create the histogram with binwidth = 1 and overlay the Poisson density.
  p <- ggplot(lp_emissions, aes(x = .data[[var]])) +
    geom_histogram(breaks = seq(min(lp_emissions[[var]], na.rm = TRUE), 
                                max(lp_emissions[[var]], na.rm = TRUE) + 1, by = 1),
                   fill = "lightblue", color = "black", alpha = 0.7) +
    stat_function(fun = function(x) dpois(x, lambda) * nrow(lp_emissions),
                  color = "red", size = 1) +
    labs(title = paste(var, "- mean =", round(lambda, 2)),
         x = var,
         y = "Frequency") +
    theme_minimal()
  
  print(p)  # Print each plot individually
}



```




